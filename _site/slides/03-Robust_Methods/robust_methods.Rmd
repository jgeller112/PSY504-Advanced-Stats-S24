---
title: "PSY 504: Advanced Statistics"
subtitle: "Robust Methods"
institute: "Princeton University"
author: "Jason Geller, Ph.D. (he/him/his)"
date: 'Updated:`r Sys.Date()`'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: true
      background-image: url("lover.png")
      background-size: cover
---
```{r xaringan-extra-styles, echo=FALSE}
library(xaringanExtra)
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)
```


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=5, fig.retina=3,
  out.width = "80%",
  tidy.opts=list(width.cutoff=80),tidy=TRUE, 
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  fig.show = TRUE,
  hiline = TRUE
)
hook_source <- knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(x, options) {
  x <- stringr::str_replace(x, "^[[:blank:]]?([^*].+?)[[:blank:]]*#<<[[:blank:]]*$", "*\\1")
  hook_source(x, options)
})
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_solarized_dark(
  header_font_google = google_font("Work Sans"),
  header_h1_font_size = "36px",
  header_color = "black",
  text_font_google = google_font("Work Sans"),
  text_font_size = "28px",
  text_color = "black", 
  background_color = "white", 
  code_font_google = google_font("Share Tech Mono"),
  extra_css = list(
    ".remark-slide-content h2" = list(
      "margin-top" = "2em",
      "margin-bottom" = "2em"
    ),
    .big = list("font-size" = "150%"),
    .small = list("font-size" = "75%"),
    .subtle = list(opacity = "0.6"),
    ".countdown-has-style h3, .countdown-has-style h3 ~ p, .countdown-has-style h3 ~ ul" = list(
      "margin" = "0"
    ),
    ".countdown-has-style pre" = list(
      "margin-top" = "-10px"
    ),
    "p .remark-inline-code" = list(
      "background-color" = "white",
      "padding" = "2px 2px",
      "margin" = "0 -2px"
    ),
    blockquote = list("margin-left" = 0),
    "em" = list(color = "#2aa198")
  ),
)
```

```{r, echo=FALSE}
library(parameters)
library(effectsize) 
library(papaja)
library(tidyverse)
library(performance)
library(see)
library(equatiomatic)
library(kableExtra)
library(broom)
library(report)
library(emmeans)
library(flextable)
library(huxtable)
library(skimr)
library(papaja)
library(moderndive)
library(tidyverse)
library(fivethirtyeight)
library(broom)
library(ggdist)
```

#  The Problem

  - Data are most often not normally distributed
  
  - Outliers
  
  - Heterogenity 

.pull-right[

```{r, echo=FALSE}

soccer <- WRS2::eurosoccer %>%
  select(League, GoalsGame) %>%
  filter(League=="Spain" | League=="Germany")


# visualize the distribution of the continuous variable for each group using a boxplot
ggplot(spider, aes(x = Group, y = Anxiety)) +
  geom_bar(stat="identity")

```
]

--

.pull-left[

```{r, echo=FALSE, fig.align='center', out.width="100%"}
# visualize the distribution of the continuous variable for each group using a boxplot with jittered raw data points
# and highlighting the outlier with a red square
ggplot(soccer, aes(x = League, y = GoalsGame, fill=League)) +
     geom_boxplot() +
     geom_point(position = "jitter", size=2) +
  theme(legend.position = "none")

```
]

---
# Population Paramter Estimate

We saw last semester: 

- Best estimate of a population mean 

$$ \mu = \bar X$$

- Estimate of sd of sampling distribution of means is the standard error of mean

$$ SE_{mean} = \frac{s_x}{\sqrt(n)}$$
- We can use this to generate 95% CIs

$$ \bar X +- t_{\alpha}(s_{\bar X})$$
---
# Bias

- Extreme scores and non-normality can bias our results

  - Bias population parameters:
  
    - $\mu$ 
    - $\sigma_2$ 
    - $SE_M$ 
  
- What can we do to reduce it?

  - Transforming data?
  
--

  - Trimming the data
  
--

  - Winsorizing
  
--
  - Bootstrapping/permutation tests
---
# Trimming Data

- Order Data

  - Remove highest and lowest 5%, 10%, or 20% 
  
- can use base `mean` function with trim argument `mean(x, trim=.2)`

```{r, echo= FALSE, fig.align='center', out.width="100%"}
knitr::include_graphics("images/trim.jpg")
```

---
# Back to our example

- What happens if we trim 20% of variable A?

```{r}

soccer %>%
  group_by(League) %>%
  summarise(mean=mean(GoalsGame),  trim_20=mean(GoalsGame, trim=.2))

```
---
# Winsorizing

- Instead of removing points, we can replace them with nearest score

.pull-left[
```{r, echo= FALSE, fig.align='center', out.width="100%"}
knitr::include_graphics("images/winsorize.jpg")
```
]

---
class: middle

# Hey Teacher, leave those means alone!
---
# Bootstrap: overview

- Computer-based method for statistical inference 

- SE, CI, and p-values constructed from the data that researcher collected... using a computer algorithm!  

  - No mathematical formula / equations involved but the ideas are sophisticated---mathematical account in seminal papers  
---
#Bootstrap: Big idea

<br>
<br>
<br>

- Using only the data at hand can sometimes give better results than making unwarranted assumptions about the variables and parameters that we are studying  
---
# Bootstrap: overview

.pull-left[
- Core mechanism: sampling with replacement 
  
  - Inference based "only" on collected data    

- Nonparametric   

- It is not magic though
]

.pull-right[
```{r, echo= FALSE, fig.align='center', out.width="100%"}
knitr::include_graphics("images/bootstrap.jpg")
```
]
---
# Percentile Bootstrap 

- Using the bootstrap sample we can calculate 95% CI 

  1. Sort values
  2. Find the 2.5th and 97.5th percentiles of the distribution of the statistic
  
```{r, echo= FALSE, fig.align='center', out.width="100%"}
knitr::include_graphics("images/bootstrap.jpg")
```

---
# Bias corrected and accelerated (Bca) bootstrap confidence interval  

- Percentile method good when using robust estimates, but... 

  1. Biased  (too wide or narrow)
  
  2. Skewed

---

- Bias-correction

$$\hat{z}_0 = \Phi^{-1}\left( \frac{1}{R+1} \sum_{r = 0}^R I( \hat{\theta}{}_r^* < \hat{\theta})  \right) \qquad \mbox{and} \qquad \hat{\gamma} = \frac{\sum_{i = 1}^n \left( \hat{\theta}_{(\cdot)} - \hat{\theta}_{(i)} \right)^3}{6 \left[\sum_{i = 1}^n \left( \hat{\theta}_{(\cdot)} - \hat{\theta}_{(i)} \right)^2\right]^{3/2}}$$

$$\phi^-1 (.975)= 1.96$$

$$\hat \theta_{r}$$  = Mean of bootstraps

$$\hat \theta$$  = Mean of sample

$$ B $$ = # of bootstraps

zË†0 measures median bias of Î¸Ë†âˆ—
, i.e., difference between
median(Î¸Ë†âˆ—b) and Î¸Ë†

---

- Skew
$$\qquad \hat{\gamma} = \frac{\sum_{i = 1}^n \left( \hat{\theta}_{(\cdot)} - \hat{\theta}_{(i)} \right)^3}{6 \left[\sum_{i = 1}^n \left( \hat{\theta}_{(\cdot)} - \hat{\theta}_{(i)} \right)^2\right]^{3/2}}$$

- Where

- ðœƒÌ‚(â‹…) is the mean of the bootstrap estimates 
- ðœƒÌ‚(ð‘–) the estimate after deleting the ð‘–th case (jacknifing)

---
# BCa In R

```{}
bcafun <- function(x,nboot,theta,R,nboot alpha=0.05){
theta.hat = theta(x)# mean
mean_fun = function(data, indices) {
  return(mean(data[indices]))
}
nx = length(x) # length of x 
boot = boot(x,mean_fun, R)
boot_se = se(boot$t)
jack = jackknife(x, mean) # jacknife it
jack_se = jack$jack.se
z0 = qnorm(sum(boot$t0<theta.hat)/nboot)
atop = sum((mean(jse$theta)-jse$theta)^3)
abot = 6*(((jse$se^2)*nx/(nx-1))^(3/2))
ahat = atop/abot
alpha1 = pnorm(z0+(z0+qnorm(alpha))/(1-ahat*(z0+qnorm(alpha))))
alpha2 = pnorm(z0+(z0+qnorm(1-alpha))/(1-ahat*(z0+qnorm(1-alpha))))
confpoint = quantile(bse$theta,probs=c(alpha1,alpha2))
list(confpoint=confpoint,z0=z0,acc=ahat,u=(jse$theta-theta.hat),
theta=bse$theta,se=bse$se)
}

set.seed(666)

boot1 <- boot(x, mean_fun, R=1000)

# a 

z0 <- qnorm(sum(boot1$t <= mean(boot1$t))/1000)

#jackknife throws x out and calulates mean then does it again and again 

jack = jackknife(x, mean) # jacknife it

# Compute the acceleration estimate
a <- sum((jack$jack.values - mean(jack$jack.values))^3) / (6 * sum((jack$jack.values - mean(jack$jack.values))^2))^3/2

alpha1 = pnorm(z0+(z0+qnorm(alpha))/(1-a*(z0+qnorm(alpha))))

alpha2 = pnorm(z0+(z0+qnorm(1-alpha))/(1-a*(z0+qnorm(1-alpha))))

alpha.l <- pnorm(z0+(z0 + qnorm(alpha))/(1-a*(z0 + qnorm(alpha))))
    
alpha.u <- pnorm(z0+(z0 + qnorm(1-alpha))/(1-a*(z0 + qnorm(1-alpha))))
    
confint.bca <- quantile(boot1$t, probs = c(alpha.l,alpha.u))


```
---
# Bootstrapping Pros

1. You can use it to determine the probability of observing values of variables that come from any distribution

  - Including times when you have no clue which distribution describes your variable and you donâ€™t even want to guess

  - Including distributions that are not currently â€œknownâ€ to science

---

2. The lack of assumptions about a variableâ€™s distribution makes the bootstrapped probability estimates more accurate

  - Far better than trying to pigeon-hole data into a distribution that doesnâ€™t describe them

3. Minimizes the influence of outliers without trivializing the inferential value of them (when using robust estimators)

---
# Bootstrapping Cons

- It can be time intensive

  - You can use Princeton HPC servers
  
    - RStudio Cloud  
---  
# Permutation Tests

- Permutation tests

  - Extension of bootstrapping when you want to test hypotheses
  
  - Does not rely on classical probability distributions

- Find statistic of interest
  
$$H_0$$ = Drug has no effect

$$H_1$$ = Drug has an effect

  - Simulate the null

    - Exchangeability (randomly sampling)
    
      - Shuffle conditions 
  
  - Do it many times

    - How many times did you  get a test statistic as large or larger? 
    
---
# Demonstration

```{r}

knitr::include_url("https://www.jwilber.me/permutationtest/")


```
---
# Permutation Test Pros

- ** Makes very limited assumptions about distribution**
---
# Permutation Test Cons

1. Permutation tests are inefficient compared to asymptotic tests. When you have only a few observations, it might be impossible to control alpha at the level you want and still do a permutation test

2. You have to assume that both samples have the exact same distribution (same shape, center, and spread) not just the same value for one parameter 

3. A permutation test assumes that the only difference between the two groups is the random assignment

4. Permutation tests can be intensive (depending on design)
---
class: middle 
# Robust Tests in R 
---
# Trimmed Means

- Can use `mean() and trim = .2`

  - Yuen test

    - Independent and dependent samples

```{r, eval=FALSE}
library(WRS2)
# indep
WRS2::yuen(GoalsGame~ League, data = soccer)
# depenp
WRS2::yuend()

```

---
# Winsorizing

- Instead of removing points, we can replace them with nearest score

```{r, eval=FALSE}
library(datawizard)
x=c(5, 6, 8, 9, 10, 20, 45)
datawizard::winsorize(x)
```

---
# Winsorized Correlations

```{r}
# Install and load the ggplot2 and corrplot packages
##install.packages("corrplot")
library(ggplot2)
library(corrplot)

# Create a dataset with two variables that are not correlated
set.seed(0)
x <- rnorm(100)
y <- rnorm(100)

# Add an outlier to the dataset
x <- c(x, 3)
y <- c(y, 3)

# Create a scatterplot to visualize the relationship between the variables
df <- data.frame(x = x, y = y)
ggplot(df, aes(x = x, y = y)) + geom_point()

# Calculate the correlation between the variables
corr <- cor(df)
print(corr)

# Plot the correlation matrix
corrplot(corr)
```
---
# Winsorized Correlations

```{r}
#winsorized correlation 20%
correlation::correlation(df, winsorize = .2)
```

---
# Heteroskedasticity

.pull-left[
- Trimming and winsorizing are good options for non-normal data with outliers

- Not very good if error is not constant

- lm assumes equal variances 
]

.pull-right[
```{r, echo=FALSE, fig.align='center', out.width="100%"}
# Load packags
library(wooldridge)

# Load the sample
data("saving")

# Only use positive values of saving, which are smaller than income
saving <- saving %>%
  filter(sav > 0,
         inc < 20000,
         sav < inc)

# Plot
ggplot(saving, aes(x = inc, y = sav)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Annual income", y = "Annual savings")

```
]

---
# Bootstrapping in R

- Sampling with replacement

```{r message=FALSE, warning=FALSE}
library(tibble) # to make well-behaved data frames
library(ggplot2) # to plot data
library(boot) # package dedicated to bootstrap methods
library(simpleboot) # package with wrapper functions to boot, making user's life easier
```

```{r}

n <- 6 # sample size
samp <- 1:n
samp
```

```{r}
set.seed(21) # for reproducible results
boot_samp <- sample(samp, size = n, replace = TRUE) # sample with replacement
```
---
# Again

```{r}
boot_samp <- sample(samp, size = n, replace = TRUE) # sample with replacement
```


--- 
# Again

```{r}
boot_samp <- sample(samp, size = n, replace = TRUE) # sample with replacement
```
---
# Bootstrap Mean Estimates

```{r}

set.seed(666)
# define the function that will be used to calculate the mean
mean_fun = function(data, indices) {
  return(mean(data[indices]))
}

# use boot to bootstrap means 1000 times (R)
results = boot(samp, mean_fun, R=1000)

# get means
means=results$t

#plot bootstraps with histogram
hist(means)
```
---
# Visualzie Bootstrap

```{r, echo=FALSE, fig.align='center', out.width="100%"}
n.show <- 50 # show only n.show first bootstrap means
df <- tibble(x = 1:n.show, y = boot.m[1:n.show])

ggplot(df, aes(x = x, y = y)) + theme_bw() +
  geom_hline(yintercept = mean(samp), colour = "grey", size = 1) +
  # comment next line to make a scatterplot instead of a lollipop chart:
  geom_segment(aes(x=x, xend=x, y=0, yend=y)) + 
  geom_point(size=2.5, color="red", fill=alpha("orange", 0.3), alpha=0.7, shape=21, stroke=2) +
  scale_x_continuous(breaks = c(1, seq(10, 100, 10))) +
  scale_y_continuous(breaks = seq(1, 6, 1)) +
  coord_cartesian(ylim = c(0, 6)) +
  labs(x = "Bootstrap samples", y = "Bootstrap means")
```
---
# `Simpleboot`

```{r}
set.seed(666)
# use to bootstrap mean or another test statitsics
x=one.boot(samp,mean, R=1000)

hist(x)
```
---
# Percentile CIs

```{r, eval = FALSE}
alpha <- 0.05
nboot=1000
bvec <- sort(boot.m) # sort bootstrap means in ascending order
# define quantiles
low <- round((alpha/2)*nboot) # 25
up <- nboot-low # 975
low <- low+1
# get confidence interval
ci <- c(bvec[low],bvec[up])
round(ci, digits = 2)
```
---
# Percentile CIs

```{r}
#can use boot.ci to get percentile and bca CIs
set.seed(666)

results_ci = boot.ci(results, type = "perc", R=10000)

results_ci
```
---
# BCa CIs

```{r}
#can use boot.ci to get percentile and bca CIs
set.seed(666)

results_ci_bca = boot.ci(results, type = "bca", R=1000)

results_ci_bca
```
---
# Two Groups
```{r}
# Group 1
n1 <- 50
m <- 400
s <- 50
location <- log(m^2 / sqrt(s^2 + m^2))
shape <- sqrt(log(1 + (s^2 / m^2)))
g1 <- rlnorm(n1, location, shape)

# Group 2
n2 <- 70
m <- 500
s <- 70
location <- log(m^2 / sqrt(s^2 + m^2))
shape <- sqrt(log(1 + (s^2 / m^2)))
g2 <- rlnorm(n2, location, shape)
```
---
## Illustrate 2 groups
```{r, warning=FALSE, message=FALSE, fig.asp=0.3}
set.seed(22) # for reproducible jitter

df <- tibble(val = c(g1, g2),
             y = rep(1, n1+n2),
             gp = factor(c(rep("Group 1",n1),rep("Group 2",n2)))
             )


p <- ggplot(data = df, aes(x = val, y = y)) + theme_bw() +
  # scatterplots
  geom_jitter(height = .05, alpha = 0.3, size = 2) + 
  theme(axis.ticks.y = element_blank(),
    axis.text.x = element_text(size = 10),
    axis.title.x = element_text(size = 12),
    strip.text.x = element_text(size = 12),
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.minor.x = element_blank()) +
  scale_y_continuous(breaks = 1) +
  labs(x = "Response times (ms)") + 
  facet_grid(cols = vars(gp)) +
  coord_cartesian(xlim = c(0, 700)) + 
  scale_x_continuous(breaks = seq(0, 1000, 100))
p

```

```{r}
# bootstrap data use function from simpleboot

set.seed(1)

boot.res <- two.boot(g1, g2, FUN=mean, R=nboot, trim=0.2)

# get 95% confidence interval (percentile) 
boot.ci(boot.res, type = "perc") # get percentile bootstrap CI

```
---
# Example Data

- Research Question: 

  - Are smaller chillies hotter than longer chillies?
  
  - Pepper Joe measured the length and heat of 85 chillies peppers
  
- DV 

  - Heat of each chili (0-11)
  
- IV 

  - Length of each Chili

```{r}
library(boot)
library(simpleboot)

chili<- fread("/Users/jasongeller/Documents/03-Robust_Methods/data/chillis.csv")
```
---
# Challenge

Data: chillis.csv

- Task:

  - Create a function to run regression 
  
  - Bootstrap a regression predicting chilli heat from length

- Use 10,000 bootstraps

- Choose and use a random seed

- Calculate BCa CIs
---
# Report Bootstrap

- We hypothesized that the length of a chilli was related to its hotness. We chose to bootstrap the distributions of model parameters to account for any deviations from normality in the data. We ran 10,000 bootstrap resamples for each parameter with a random seed of 237846 using the boot package (Canty & Ripley, 2017) in R 4.0.4 (R Core Team, 2020). The estimates for all parameters are provided in Table 1. Relevant to our hypothesis, the median value for the influence of length on hotness was -.15, 95% CI [-0.27, -0.08]. This interval does not include 0, which, when taken with the sign of the slope, suggests that shorter chillies are hotter.
---
# Hypothesis Testing - Two Sample

- Simulate the null hypothesis (that drug and control labels are random)

  - Throw both groups into a bucket
  
  - Randomly reconstitute the two groups, disregarding their original group membership (**EXCHANGABILITY**)  
  
  - *(Resample without replacement)*
  
  - Recompute the statistic of interest
---
# Hypothesis Testing - Two Sample

```{r}
g_control <- c(87,90,82,77,71,81,77,79,84,86,78,84,86,69,81,75,70,76,75,93) 
g_drug <- c(74,67,81,61,64,75,81,81,81,67,72,78,83,85,56,78,77,80,79,74) # our statistic of interest here is the difference between means
stat_obs <- mean(g_control) - mean(g_drug)

stat_obs

  
```
---
```{r}
n_perm = 10000 # how many simulated experiments?
stat_perm = array(NA, n_perm) # create a list to store our permutation test values 
g_control_n = length(g_control) # length of g_control var
g_drug_n = length(g_drug) # length of g_drug var
g_bucket = c(g_control, g_drug) # combine g_control and g_drug
g_bucket_n = length(g_bucket) # length of g_bucket var

for (i in 1:n_perm) {
  # reconstitute both groups, ignoring original labels
permuted_bucket <- sample(g_bucket,g_bucket_n,replace=FALSE) # sample both
perm_control <- permuted_bucket[1:g_control_n]
perm_drug <- permuted_bucket[(g_control_n+1):(g_control_n+g_drug_n)] 
stat_perm[i] <- mean(perm_control) - mean(perm_drug)
  
}  
```

---

.pull-left[
```{r, echo=FALSE}

# visualize the empirical permutation distribution of our statistic of interest

hist(stat_perm, 50, xlab="mean(control) - mean(drug)", main="Permutation Test")
abline(v=stat_obs, col="red", lwd=2)
abline(v=-stat_obs, col="red", lwd=2)
# how many times in the permutation tests did we observe a stat_perm bigger than or smaller than the stat_obs?
p_perm0 <- length(which(stat_perm >= abs(stat_obs))) / n_perm

p_perm1 <- length(which(stat_perm <= -1*abs(stat_obs))) / n_perm

p_perm2 <- p_perm0 + p_perm1

legend(x="topright", lty=1, col="red", legend=paste("stat_obs: p = ", p_perm2))
```
]

.pull-right[
```{r, fig.align='center', out.width="100%"}

# visualize the empirical permutation distribution of our statistic of interest
hist(stat_perm, 50, xlab="mean(control) - mean(drug)", main="Permutation Test")
abline(v=stat_obs, col="red", lwd=2)
abline(v=-stat_obs, col="red", lwd=2)
# how many times in the permutation tests did we observe a stat_perm bigger than or smaller than the stat_obs?
p_perm0 <- length(which(stat_perm >= abs(stat_obs))) / n_perm
p_perm1 <- length(which(stat_perm <= -1*abs(stat_obs))) / n_perm

p_perm2 <- p_perm0 + p_perm1
legend(x="topright", lty=1, col="red", legend=paste("stat_obs: p = ", p_perm2))
```
---  
# `Permuco` R Package

```{r}

library(permuco)

#lmperm() # linear models
#aovperm() # anova models

g_control <- c(87,90,82,77,71,81,77,79,84,86,78,84,86,69,81,75,70,76,75,93) 
g_drug <- c(74,67,81,61,64,75,81,81,81,67,72,78,83,85,56,78,77,80,79,74) # our statistic of interest here is the difference between means

d <- data.frame(control=g_control, drug=g_drug) %>%
  pivot_longer(control:drug, names_to = "group")

permuco::lmperm(value~group,data=d,np=10000)
```
---