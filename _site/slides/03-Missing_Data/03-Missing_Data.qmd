---
title: "Missing Data in R"
subtitle: "Princeton University"
author: "Jason Geller, PH.D.(he/him)"
date: 'Updated:`r Sys.Date()`'
footer: "PSY 504: Advaced Statistics"
format: 
  revealjs:
    theme: blood
    css: slide-style.css
    multiplex: true
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    fontsize: "25pt"
    highlight-style: github-dark
execute:
  freeze: auto
  echo: true
  message: false
  warning: false
  fig-align: center
  fig-width: 16
  fig-height: 10
  editor_options: 
  chunk_output_type: inline
  code-overflow: wrap
  html:
    code-fold: true
    code-tools: true
bibliography: references.bib
---

## Today

::: columns
::: {.column width="50%"}
-   MCAR, MAR, NMAR

-   Screening data for missingness

-   Diagnosing missing data mechanisms in R

-   Missing data methods in R

    -   Listwise deletion
    -   Casewise deletion
    -   Nonconditional and conditional imputation
    -   Multiple imputation
    -   Maximum likelihood

-   Reporting
:::

::: {.column width="50%"}
![Richard McElreath](images/terminator.png){fig-align="center" width="296"}
:::
:::

## Packages

Install the **mice** package

```{r, eval = F}
install.packages("mice")
```

Load these packages:

```{r, warning = F, message = F}
library(tidyverse) 
library(easystats)
library(knitr)
library(broom) #tidy statistics
library(ggmice) #graph missing data
library(mice) # dealing and visualizing missing data
library(naniar) # missing data + visualization
library(finalfit)# missing data visualization

```

Here is the link to the .qmd document to follow along: <https://github.com/jgeller112/PSY504-Advanced-Stats-S24/blob/main/slides/03-Missing_Data/03-Missing_Data.qmd>.

## Missing data mechanisms

::: columns
::: {.column width="50%"}
-   Most of modern missing data theory comes from the work of statistician Donald B. Rubin

-   Rubin proposed we can divide an entire data set $Y$ into two components:

    -   $Y_\text{obs}$ the observed values in the data set

    -   $Y_\text{mis}$ the missing values in the data set

$$Y = Y_\text{obs} + Y_\text{mis}$$
:::

::: {.column width="50%"}
<br>

<br>

![](images/Screen%20Shot%202024-01-04%20at%207.46.41%20AM.png){fig-align="center"}
:::
:::

## Missing data mechanisms

-   Missing data mechanisms (processes) describe different ways in which the data relate to nonresponse

-   Missingness may be completely random or systematically related to different parts of the data

-   Mechanisms function as statistical assumptionsÂ 

## Missing completely at random (MCAR)

::: columns
::: {.column width="50%"}
-   The probability of missingness is unrelated to the data

-   MCAR is purely random missingness
:::

::: {.column width="50%"}
![](images/mcar.png){fig-align="center"}
:::
:::

## Conditionally missing at random (CMAR)

::: columns
::: {.column width="50%"}
-   Systematic missingness related to the observed scores

-   The probability of missing values is unrelated to the unseen (latent) data
:::

::: {.column width="50%"}
![](images/cmar.png){fig-align="center"}
:::
:::

## Not missing at random (NMAR)

::: columns
::: {.column width="50%"}
-   Systematic missingness

-   The probability of missing values is related to the unseen (latent) data
:::

::: {.column width="50%"}
![](images/nmar.png){fig-align="center"}
:::
:::

# Data

## Chronic pain example

-   Enders (2023)

    -   Study (*N* = 275) investigating psychological correlates of chronic pain

        -   Depression (`depress`)
        -   Perceived control (`control`)

-   Perceived control over pain is complete, depression scores are missing

    ::: callout-note
    I manipulated the dataset so missingness is related to control over pain (i.e., low control is related to missingness on depression scores)
    :::

## Chronic pain example

::: columns
::: {.column width="50%"}
```{r}

dat <- read.table("https://raw.githubusercontent.com/jgeller112/PSY504-Advanced-Stats-S24/main/slides/03-Missing_Data/APA%20Missing%20Data%20Training/Analysis%20Examples/pain.dat", na.strings = "999")

names(dat) <- c("id", "txgrp", "male", "age", "edugroup", "workhrs", "exercise", "paingrps", 
                "pain", "anxiety", "stress", "control", "depress", "interfere", "disability",
                paste0("dep", seq(1:7)), paste0("int", seq(1:6)), paste0("dis", seq(1:6)))

dat <- dat  %>%
  select("id", "age", "control",  "depress", "stress") %>%
  mutate(depress=ifelse(depress==999, NA, depress)) %>%
    mutate(r_mar_low = ifelse(control < 15.51, 1, 0)) %>% 
  mutate(depress = ifelse(r_mar_low == 1, NA, depress)) %>%
  select(-r_mar_low)

```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| 
dat <- dat  %>%
  select("id", "age", "control",  "depress", "stress") %>%
  mutate(depress=ifelse(depress==999, NA, depress)) %>%
    mutate(r_mar_low = ifelse(control < 15.51, 1, 0)) %>% 
  mutate(depress = ifelse(r_mar_low == 1, NA, depress)) %>%
  select(-r_mar_low)

dat %>%
  kable()

```
:::
:::

## Exploratory data analysis (EDA)

-   Look at your data
-   Need to identify missing data!
    -   Explore data using descriptive statistics and figures

        -   What variables have missing data? How much is missing in total? By variable?

## EDA

```{r}
#| fig-align: "center"
library(naniar)
vis_miss(dat)
```

## EDA

```{r}

library(skimr)
skimr::skim(dat) %>% 
  kable()

```

## Missing patterns

::: columns
::: {.column width="50%"}
-   A missing data pattern matrix is the structure of observed and missing values in the dataset
    -   Where the data is missing

```{r}
#md.pattern(dat)
#ggmice
dat %>%
 # create missing data pattern plot
plot_pattern()

```
:::

::: {.column width="50%"}
<br>

<br>

<br>

```{r}
#| echo: false
#| fig-align: "center"
knitr::include_graphics("images/missing_patterns.png")
```
:::
:::

## Missing patterns

::: columns
::: {.column width="50%"}
-   Univariate: one variable with missing data

-   Monotone: patterns in the data can be arranged

    -   Associated with a longitudinal studies where members drop out and never return

-   Non-monotone: missingness of one variable does not affect the missingness of any other variables

    -   Look for islands of missingness
:::

::: {.column width="50%"}
```{r}
#| fig-align: "center"
#| echo: false
#| 

knitr::include_graphics("images/new_patterns.webp")
```
:::
:::

## Is it MCAR or MAR?

![](images/marcar.png){fig-align="center"}

## Is it MCAR or MAR?

-   Can make a case for MCAR

    -   Little's test

        -   $\chi^2$

            -   Sig = not MCAR

            -   Not sig = MCAR

```{r}
library(naniar)

mcar_test(dat) %>% kable()
```

-   Not used much anymore!

## Is it MCAR or MAR?

```{r, echo=TRUE, fig.align='center', out.width="100%"}
library(finalfit)

explanatory = c("control", "age")
dependent = "depress" 

misspairs <- dat %>%
  missing_pairs(explanatory, dependent)
   # mice function visualize data
```

```{r, echo=FALSE, fig.align='center', out.width="100%"}
misspairs
```

## Is it MCAR or MAR?

-   Our job is to find out if our data is MAR

    -   Create a dummy coded variable for missing variable where 1 = score missing and 0 = score not missing on missing variable

        -   If these variables are related to other variables in dataset

            -   MAR

```{r}
pain_r <- dat %>%#can also use case_when #if missing 1 else 
  mutate(depress_1 = ifelse(is.na(depress), 1, 0))
pain_r %>% head() %>% kable()

```

## Testing

-   lm, *t*-test, or glm

```{r}
model <- lm(control~depress_1,data=pain_r)

tidy(model) %>%
  kable()
```

-   It looks like missing on depression is related to control!

# Methods for dealing with MCAR

## Listwise deletion

::: columns
::: {.column width="50%"}
```{r}
dat %>%
  kable()
```
:::

::: {.column width="50%"}
```{r}

dat  %>%
  drop_na() %>%
  kable()

```
:::
:::

## Listwise deletion: pros and cons

-   Pros:

    -   Produces the correct parameter estimates if missingness is MCAR

        -   If not, biased

-   Cons:

    -   Can result in a lot of data loss

## Casewise (pairwise) deletion

-   In each comparison, delete only observations if the missing data is relevant to this comparison

```{r, echo=TRUE}
#create data frame
dat %>%
  kable()
```

## Casewise deletion: pros and cons

Pros:

-   Avoids data loss

-   Non-biased

    -   Only for MCAR

Cons:

-   But, results not completely consistent or comparable--based on different observations

# Methods for MAR

## Unconditional (mean) imputation - Bad

-   Replace missing values with the mean of the observed values

    -   Reduces variance

        -   Increases Type 1 error rate

::: columns
::: {.column width="50%"}
```{r}

d <- c(5, 8, 3, NA, NA)

#calc mean remove NAs
d_mean <- mean(d, na.rm = TRUE)

d_mean_imp <- ifelse(is.na(d), d_mean, d) # add mean

d_mean_imp
```
:::

::: {.column width="50%"}
```{r}

sd(d, na.rm=TRUE) # sd of org dataset
sd(d_mean_imp) # sd of mean impute dataset

#NANIRE Package 
#impute_mean(d) %>% head()
# Do this in Mice
#complete(mice(data, m=1, method="mean"))
```
:::
:::

## Conditional imputation (regression)

::: columns
::: {.column width="50%"}
-   Run a regression using the complete data to replace the missing value

```{r}
#| eval: false
#| 

lm(depress~control) # on complete data

imp.regress <- mice(dat, method="norm.predict", m=1, maxit=1) # norm.predict performs regression impute


```

-   All the other related variables in the data set are used to predict the values of the variable with missing data

-   Missing scores have the predicted values provided to replace them
:::

::: {.column width="50%"}
![](images/reg_imp.png){fig-align="center"}
:::
:::

## Stochastic Regression

-   Regression imputation with added error variance to predicted values

```{r}
#| eval: false
#| 
mice(data, m=1, method="norm.nob") # perfrom stoch regression 

```

![](images/stoch_reg.png){fig-align="center"}

# MI

## Multiple Imputation

-   Instead of using one value as a true value (which ignores uncertainty and variance), we use multiple values

-   Basically doing conditional imputation several times

    -   Several steps

1.  We make several multiply imputed data sets with the `mice()` function

![](images/Screen%20Shot%202024-01-04%20at%205.05.58%20PM.png){fig-align="center"}

## Multiple Imputation

2.  We fit our model of choice to each version of the data with the `with()` function

![](images/Screen%20Shot%202024-01-04%20at%205.06.45%20PM.png){fig-align="center"}

## Multiple Imputation

3.  We then pool (i.e., combine) the results with the `pool()` function

![](images/Screen%20Shot%202024-01-04%20at%205.07.17%20PM.png){fig-align="center"}

## Multiple Imputation

![](images/mice_comp.PNG){fig-align="center" width="666"}

![](){fig-align="center"}

## 1. Impute with `Mice`

```{r}
m=5
# impute several data sets
imp <- mice(dat, m = m, seed = 24415, method="pmm", print = FALSE)

```

-   What is `imp`?

```{r}
str(imp, max.level = 1)
```

## 1. Impute with `Mice`

-   What is `imp` within `imp`?

```{r}
# uses all data to imputet even ID so change accordingly
str(imp$imp, max.level = 1)

#get the imputed values for that var
head(imp$imp$depress)

#get the imputed datasets out
#complete(imp, "all")

```

## PMM

::: columns
::: {.column width="50%"}
-   Predictive mean matching

    -   For each missing value, a regression model is fitted using the observed (complete) data, where the variable with missing data is the outcome and other variables are predictors
    -   For a record with a missing value, the fitted model predicts a mean based on the available data
:::

::: {.column width="50%"}
-   PMM identifies a set of "donors" from the observed data. These donors are the cases whose predicted means are closest to the predicted mean of the case with the missing value

![](images/PMM.PNG){width="435" height="319"}
:::
:::

## 2. Model with `Mice`

-   We'll fit a simple statistical model

```{r}
#fit the model to each set of imputaed data

fit <- with(data = imp, expr = lm(depress ~ control))

summary(fit) %>%
  kable()
```

## 3. `Mice` Pool Results

```{r}
#combine the results
result <- pool(fit)

model_parameters(result) %>%
  kable()
```

## 3. `Mice` Pool Results

-   `emmeans` does not play nicely with `mice` objects

-   `marginaleffects`

    -   Can be used to perform hypothesis tests on coefficients

```{r}
library(marginaleffects)

mfx_mice <- avg_slopes(fit) # avg_comparions for categorical vars

mfx_mice %>% kable()

```

## Plot Imputations

-   Make sure they look similar to real data

```{r, fig.align='center', out.width="60%"}
# create stripplot 
ggmice(imp, ggplot2::aes(x = .imp, y = depress)) +
  ggplot2::geom_jitter() + 
    labs(x = "Imputation number")
```

# Maximum likelihood (ML)

## ML

-   Determines the most probable settings (parameter estimates) for a statistical model by making the model's predicted outcomes as close as possible to the observed data

-   Each observationâ€™s contribution to estimation is restricted to the subset of parameters for which there is dataÂ 

-   Estimation uses incomplete data, no imputation performedÂ 

## ML

Implicit imputation

-   Each participant contributes their observed data

-   Data are not filled in, but the multivariate normal distribution acts like an imputation machine

-   The location of the observed data implies the probable position of the unseen data, and estimates are adjusted accordinglyÂ 

![](images/Screen%20Shot%202024-01-04%20at%205.30.44%20PM.png){fig-align="center" width="538"}

## Chronic pain illustration

-   Participants with low perceived control are more likely to have missing depression scores (conditionally MAR)

-   The true means are both 20

![](images/Screen%20Shot%202024-01-04%20at%205.31.57%20PM.png){fig-align="center" width="598"}

## Deleting incomplete information

::: columns
::: {.column width="50%"}
-   Deleting cases with missing depression scores gives a non-representative sample

-   The perceived control mean is too high (Mpc = 23.1), and the depression mean is too low (Mdep = 17.2)Â 
:::

::: {.column width="50%"}
![](images/Screen%20Shot%202024-01-04%20at%205.34.18%20PM.png){fig-align="center" width="607"}
:::
:::

## Partial data

-   Incorporating the partial data gives a complete set of perceived control scores

-   The partial data records primarily have low perceived control scoresÂ 

![](images/Screen%20Shot%202024-01-04%20at%205.35.03%20PM.png){fig-align="center" width="605"}

## Adjusting perceived control

-   Adding low perceived control scores increases the variable's variability

-   The perceived control mean receives a downward adjustment to accommodate the influx of low scoresÂ 

![](images/Screen%20Shot%202024-01-04%20at%205.36.26%20PM.png){fig-align="center"}

## Implicit Imputation

-   Maximum likelihood assumes multivariate normality

-   In a normal distribution with a negative correlation, low perceived control scores should pair with high depressionÂ 

![](images/Screen%20Shot%202024-01-04%20at%205.37.37%20PM.png){fig-align="center"}

## Adjusting Depression Distribution

-   Maximum likelihood intuits the presence of the elevated but unseen depression scores

-   The mean and variance of depression increase to accommodate observed perceived control scores at the low endÂ 

![](images/ML_unbiased.png){fig-align="center" width="559"}

## ML: Cons

-   Generally limited to normal data, options for mixed metrics are less common

-   Normal-theory methods are biased with interactions and non-linear terms

-   MLM software usually discards observations with missing predictorsÂ 

## Distinguish between NMAR and MAR

-   Pray you don't have to ðŸ˜‚

-   It's complicated

    -   Not many good techniques

-   NMAR into MAR

    -   Try and track down the missing data

    -   Auxiliary variables

    -   Collect more data for explaining missingness

## Distinguish between NMAR and MAR

![](images/reddit.jpg){fig-align="center"}

## Reporting Missing Data

-   Template from [Stepf van Buuren](https://stefvanbuuren.name/fimd/sec-reporting.html)

::: callout-tip
The *percentage of missing values* across the nine variables varied between *0 and 34%*. In total *1601 out of 3801 records (42%)* were incomplete. Many girls had no score because the nurse felt that the measurement was "unnecessary," or because the girl did not give permission. Older girls had many more missing data. We used *multiple imputation* to create and analyze *40 multiply imputed datasets*. Methodologists currently regard multiple imputation as a state-of-the-art technique because it improves accuracy and statistical power relative to other missing data techniques. *Incomplete variables were imputed under fully conditional specification, using the default settings of the mice 3.0 package (Van Buuren and Groothuis-Oudshoorn 2011)*. The parameters of substantive interest were estimated in each imputed dataset separately, and combined using Rubin's rules. For comparison, *we also performed the analysis on the subset of complete cases.*
:::

## Report

-   *Amount of missing data*
-   *Reasons for missingness*
-   *Consequences*
-   *Method*
-   *Imputation model*
-   *Pooling*
-   *Software*
-   *Complete-case analysis*

## Is it MCAR, MAR, NMAR?

> The post-experiment manipulation-check questionnaires for five participants were accidentally thrown away.

. . .

-   MCAR

## Is it MCAR, MAR, NMAR?

> In a 2-day memory experiment, people who know they would do poorly on the memory test are discouraged and don't want to return for the second session

. . .

-   NMAR

## Is it MCAR, MAR, NMAR?

> A health psychologist is surveying high school students on marijuana use. Students who scored highly on anxiety left these questions blank.

. . .

-   MAR

## Wrap up

-   When you have missing data, think about WHY they are missing

-   Missing data handled improperly can bias your expectations

-   MI and ML are good ways to handle missing data!

    -   Bayesian methods are good too :)

    -   Inverse probability weighting seem to work well [@gomila2022]
