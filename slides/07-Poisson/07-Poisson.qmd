---
title: "Poisson (üêü), Negative Binomial, and Zero-Inflated Models"
subtitle: "Princeton University"
author: "Jason Geller, PH.D.(he/him)"
date: 'Updated:`r Sys.Date()`'
footer: "PSY 504: Advanced Statistics"
format: 
  revealjs:
    theme: blood
    css: slide-style.css
    multiplex: true
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    fontsize: "25pt"
webr:
  packages: ["tidyverse", "easystats", "broom", "knitr", "emmeans", "ggeffects", "lme4", "pscl", "glmmTMB"]
filters:
  - webr
execute:
  freeze: auto
  echo: true
  message: false
  warning: false
  fig-align: center
  fig-width: 12
  fig-height: 8
  editor_options: 
  chunk_output_type: inline
  code-overflow: wrap
  html:
    code-fold: true
    code-tools: true
---

## Today

-   GLMs for count data

    -   Poisson, negative binomial (overdispersion), and zero-inflated (excess zeros) models

        -   Fitting and interpretation in R

        -   Visualization

        -   Effect sizes

        -   Reporting

    -   Running example: Stuttering and fixations to faces

## Packages

```{r}
library(tidyverse) # data wrangling
library(emoji) # emojis 
library(easystats) # performance
library(kableExtra) # table formatting
library(ggeffects) # plot 
library(lme4) # glmer 
library(knitr)
library(patchwork) # combine plots
library(viridis) # colors
library(gridExtra)
library(pscl) # zip 
library(glmmTMB) # zero inflated glmer 

options(scipen=999) # get rid of sci notation
```

-   You can follow along [here](https://github.com/jgeller112/PSY504-Advanced-Stats-S24/blob/main/slides/07-Poisson/07-Poisson.qmd)

## Count data

-   Today's lecture brought to you by the number 8

```{r, echo=F, out.width="100%", fig.align='center'}

knitr::include_graphics("images/count.jpeg")
```

## Poisson distribution {data-link="Poisson distribution"}

```{r}
#| echo: false
#| fig-align: "center"
#| 

set.seed(2000)
sim1 <- rpois(100000,1)
sim2 <- rpois(100000,5)
sim3 <- rpois(100000,50)
pois_sim <- tibble (
  sim1 = sim1, 
  sim2 = sim2, 
  sim3 = sim3
)

ggplot(data = pois_sim, aes(x = sim1)) +
  geom_histogram() +
  labs(x = "", title = "lambda:1") + 
  theme_lucid(base_size=25)


```

## Poisson distribution

$$Y_i \sim Poisson(\lambda)$$

Let $Y$ be the number of events in a given unit of time or space.

$$P(Y=y) = \frac{e^{-\lambda}\lambda^y}{y!} \hspace{10mm} y=0,1,2,\ldots, \infty$$

-   $E(Y) = Var(Y) = \lambda$ (*just the mean number of events*)
-   The distribution is typically skewed right, particularly if $\lambda$ is small
-   The distribution becomes more symmetric as $\lambda$ increases
    -   If $\lambda$ is sufficiently large, it can be approximated using a normal distribution

## Poisson distribution

```{r}
#| fig-align: "center"
#| echo: false
#| 
set.seed(2000)
sim1 <- rpois(100000,1)
sim2 <- rpois(100000,5)
sim3 <- rpois(100000,50)
pois_sim <- tibble (
  sim1 = sim1, 
  sim2 = sim2, 
  sim3 = sim3
)
p1 <- ggplot(data = pois_sim, aes(x = sim1)) +
  geom_histogram() +
  labs(x = "", title = "lambda:1")
p2 <- ggplot(data = pois_sim, aes(x = sim2)) +
  geom_histogram() +
  labs(x = "", title = "lambda:5")
p3 <- ggplot(data = pois_sim, aes(x = sim3)) +
  geom_histogram() +
  labs(x = "", title = "lambda:50")
p1 + p2 + p3 
```

```{r echo = F}
sum1 <- c(mean(sim1), var(sim1))
sum2 <- c(mean(sim2), var(sim2))
sum3 <- c(mean(sim3), var(sim3))
data <- rbind(sum1,sum2,sum3)
rownames(data) <- c("lambda = 1", "lambda = 5","lambda = 50")
colnames(data) <- c("Mean", "Variance")
kable(data,format="html")
```

## Examples

The annual number of earthquakes registering at least 2.5 on the Richter Scale and having an epicenter within 40 miles of downtown Memphis follows a Poisson distribution with mean 6.5. **What is the probability there will be 3 or fewer such earthquakes next year?**

$$P(Y<=y) = \frac{e^{-6.5}6.5^0} {0!}+
\frac{e^{-6.5}6.5^1} {1!} + 
 \frac{e^{-6.5}6.5^2} {2!} + 
  \frac{e^{-6.5}6.5^3} {3!}$$

```{webr-r}
a=(exp(-6.5) * 6.5^0) / factorial(0)
b=(exp(-6.5) * 6.5^1) / factorial(1)
c=(exp(-6.5) * 6.5^2) / factorial(2)
d=(exp(-6.5) * 6.5^3) / factorial(3)

ppois(3, 6.5)
```

## Examples

-   Exact count

    -   Let's say you read, on average, 10 pages an hour. **What is the probability you will read 8 pages in an hour?**

. . .

$$P(Y=y)= \frac{e^{-10}10^8} {8!}$$

```{webr-r}

prob <- (exp(-10) * 10^8) / factorial(8)

dpois(x=8, lambda=10)

prob

```

# Poisson regression

## Preferential viewing task

-   The data: viewing behavior to emotional faces

```{r, fig.align='center', echo=FALSE}

include_graphics("images/faces.png")

```

## Preferential viewing task

**Response**:

-   Number of fixations to each face

-   **Predictors**:

    -   `Emotion`: Anger vs. Happy (within-subject)
    -   `Group`: Control vs. Stuttering (between subject)

## The data

```{webr-r}
hh_data <- read.csv("https://raw.githubusercontent.com/jgeller112/psy504-advanced-stats/main/slides/Poisson/data/tobii_aoi_study1.csv")
```

```{r}
#| echo: false
hh_data <- read.csv("https://raw.githubusercontent.com/jgeller112/psy504-advanced-stats/main/slides/Poisson/data/tobii_aoi_study1.csv")
```

## The data

```{r, echo=TRUE}
hh_data <- hh_data %>% dplyr::select(ID, Number_of_fixations, emotion, Group) %>%
  filter(emotion=="Anger"| emotion=="Happy")

head(hh_data) %>%
  kable()
  
```

## Response variable

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| 
ggplot(data = hh_data, aes(x = Number_of_fixations)) +
  geom_histogram() + 
  labs(title = "Total number of fixations")
```
:::

::: {.column width="50%"}
```{r}

hh_data %>% ungroup() %>%
  summarise(mean = mean(Number_of_fixations), var = var(Number_of_fixations), ratio=mean/var) %>%
  kable(digits = 3)


```
:::
:::

## Why the least-squares model doesn't work

The goal is to model $\lambda$, the expected number of fixations on faces, as a function of the predictors (covariates)

. . .

We might be tempted to try a linear model $$\lambda_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{ip}$$

. . .

This model won't work because...

-   It could produce negative values of $\lambda$ for certain values of the predictors

-   The equal variance assumption required to conduct inference for linear regression is violated

## Poisson regression model

If $Y_i \sim Poisson$ with $\lambda = \lambda_i$ for the given values $x_{i1}, \ldots, x_{ip}$, then

\
$$\log(\lambda_i) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}$$

. . .

-   Each observation can have a different value of $\lambda$ based on its value of the predictors $x_1, \ldots, x_p$

-   $\lambda$ determines the mean and variance, so we don't need to estimate a separate error term

## Assumptions for Poisson regression

-   **Poisson response**: The response variable is a count per unit of time or space, described by a Poisson distribution, at each level of the predictor(s)

-   **Independence**: The observations must be independent of one another

-   **Linearity**: The log of the mean rate, $\log(\lambda)$, must be a linear function of the predictor(s)

-   **Mean = Variance**: The mean must equal the variance

# Poisson regression: Fitting and Interpretation {.smaller}

## Poisson regression: Fitting and Interpretation

```{webr-r}

hh_data_contrast <- hh_data %>%
  mutate(emotion=ifelse(emotion=="Anger", 0.5, -0.5), Group=ifelse(Group=="S", 0.5, -0.5))
```

```{r, echo=FALSE}
# contrast coding
hh_data_contrast <- hh_data %>%
  mutate(emotion=ifelse(emotion=="Anger", 0.5, -0.5), Group=ifelse(Group=="S", 0.5, -0.5))
```

-   `glm`

```{r, eval=TRUE}
# regular glm 
model_glm <- glm(Number_of_fixations ~ emotion*Group, data = hh_data, family = poisson(link = "log")) # change family to poisson

```

-   `glmer`

```{r}
#| echo: false
model1<- glmer(Number_of_fixations ~ emotion+ Group + (1|ID), data = hh_data, family = poisson(link = "log")) # change family to poisson
```

```{r}
#| echo: false
#| 
# fit poisson model# change family to poisson
# repeated measures poisson
model1_cont <- glmer(Number_of_fixations ~ emotion+ Group + (1|ID), data = hh_data_contrast, family = poisson(link = "log")) # change family to poisson
```

```{webr-r}
# fit poisson model# change family to poisson
# repeated measures poisson
model1_cont <- glmer(Number_of_fixations ~ emotion+ Group + (1|ID), data = hh_data_contrast, family = poisson(link = "log")) # change family to poisson

```

## Poisson regression: Fitting and interpretation

-   Coefficients represent log lambda

```{r}
parameters::model_parameters(model1_cont, exponentiate = FALSE, effects="fixed") %>% kable(digits = 3, format = "markdown")
```

## Poisson regression: Fitting and interpretation

-   Mean counts = more interpretable

```{r, fig.align='center', echo=FALSE}

knitr::include_graphics("images/log_poi.png")

```

## Poisson regression: Fitting and interpretation

-   Incidence rate ratios (IRR)

    -   The IRR for a one-unit change in $x_i$ is exp $(\beta)$

    -   The coefficient tells you how changes in X affect the rate at which Y occurs

::: callout-note
IRR \> 1: Expected \# events increases for 1 unit increase

IRR \< 1: Expected \# events decreases for 1 unit increase

IRR = 1: No difference in expected number of events
:::

## Poisson regression: Fitting and interpretation

```{r, echo=FALSE}
model_parameters(model1_cont, exponentiate = FALSE, effects="fixed") %>%
  print_md()
```

-   exp($\alpha$) = Overall mean count

```{webr-r}

```

## Poisson regression: Fitting and interpretation

::: panel-tabset
## Log

```{r}
#| eval: false
#| 

model_parameters(model1_cont, effects="fixed")

```

```{r}
#| echo: false
model_parameters(model1_cont, effects="fixed") %>% 
 print_md() %>% kableExtra::row_spec(2, bold= T, color="red", background = "white")
```

## Exp

```{r}
#| eval: false
model_parameters(model1_cont, exponentiate = TRUE, effects="fixed")

```

```{r}
#| echo: false

model_parameters(model1_cont, exponentiate = TRUE, effects="fixed") %>% 
  print_md()  %>% kableExtra::row_spec(2, bold= T, color="red", background = "white")
```
:::

::: callout-important
-   Angry faces have .08 fewer log fixations than happy faces or
    - exp(-.08) = `r round(exp(-.08), 3)`x that of happy faces. The expected number of fixations is 8% lower for angry faces vs. happy faces
:::

## Main effect: `emotion`

-   Marginal mean counts for each condition

```{r}
marginaleffects::avg_predictions(model1, variables="emotion") %>%
  kable()
```

## Marginal Effects

-   Difference in counts

```{r}

marginaleffects::avg_comparisons(model1, variables = "emotion") %>%
  kable()

```

::: callout-important
Happy faces had .33 more fixations than angry faces
:::

## Poisson regression: Fitting and interpretation

::: panel-tabset
## Log

```{r}
#| eval: false
#| 
model_parameters(model1_cont, effects="fixed")

```

```{r}
#| echo: false
#| 
model_parameters(model1_cont, effects="fixed") %>% 
  print_md() %>% kableExtra::row_spec(., row=3, bold= T, color="red", background = "white")

```

## Exp

```{r}
#| eval: false
#| 
model_parameters(model1_cont,exponentiate = TRUE,  effects="fixed")
```

```{r}
#| echo: false
model_parameters(model1_cont, exponentiate = TRUE, effects="fixed") %>% 
  print_md()  %>% kableExtra::row_spec(., row=3, bold= T, color="red", background = "white")

```
:::

::: callout-important
-   Stuttering group has .09 more log fixations than Control group, or

    -   exp(0.85) = `r round(exp(.09), 3)`x that of Control group. The expected number of fixations is 9% higher for Control vs. stuttering
:::

## Main effect: `Group`

-   Marginal mean counts

```{r}
marginaleffects::avg_predictions(model1, variables="Group") %>% 
  kable(digits = 3, format = "markdown")

```

## Marginal Effects

-   Difference in counts

```{r}

marginaleffects::avg_comparisons(model1, variables = "Group") %>%
  kable()
```

::: callout-important
Stuttering group had .33 more fixations than Control group.
:::

## Full model

-   LRT test for more complex models

```{r}
#overall model
mod_1 <- car::Anova(model1, type="II")

mod_1 %>% kable(digits = 3, format = "markdown")
```

## Model 2: Add interaction

```{r}
#| echo: false
#| 
model2_dum <- glmer(Number_of_fixations ~ emotion*Group+ (1|ID), data = hh_data, family = poisson)

```

```{r}
model2 <- glmer(Number_of_fixations ~ emotion*Group+ (1|ID), data = hh_data_contrast, family = poisson)

model_parameters(model2, effects="fixed") %>% 
   print_md() %>% kableExtra::row_spec(., row=4, bold= T, color="red", background = "white")

```

## Add `emotion*Group` to the model?

-   `r emoji("check")` Conduct a drop-in-deviance LR test

```{r}
test_likelihoodratio(model1_cont, model2) %>% 
  kable()
```

. . .

-   `r emoji("check")` Yes!

## Interaction: `Group*emotion`

-   Simple effects test done on the *response* (count)

```{r}

marginaleffects::avg_comparisons(
    model_glm,
    variables  = "Group", 
    by="emotion") %>%
  kable()

```

. . .

-   More fixations to Angry faces vs. Happy faces for the Stuttering group

## Visualizing poisson regression

-   Used expected/predicted values

```{r, fig.align='center', out.width="60%"}
ggemmeans(model2_dum, terms=c("emotion", "Group")) %>%
  plot(show_data=TRUE, colors = "hero", jitter=.2)
```

## Assumptions

```{r, fig.align="center", out.width="60%"}
performance::check_model(model2, check = c("pp_check", "outliers", "vif", "overdispersion"))
```

## Overdispersion

**Overdispersion**: There is more variability in the response than what is implied by the Poisson model

```{r echo = F}
hh_data %>%
  summarise(mean = mean(Number_of_fixations), var = var(Number_of_fixations)) %>%
  kable(digits = 3)
```

```{r echo = F}
hh_data %>%
  group_by(emotion) %>%
  summarise(mean = mean(Number_of_fixations), var = var(Number_of_fixations), ratio=mean/var) %>%
  kable(digits = 3)

```

```{r, echo=F}

hh_data %>%
  group_by(Group) %>%
  summarise(mean = mean(Number_of_fixations), var = var(Number_of_fixations), ratio=mean/var) %>%
  kable(digits = 3)

```

## Testing for overdispersion

```{r}
#| message: true
# easystats
performance::check_overdispersion(model2)
```

$$
\text{Pearson residual}_i = \frac{\text{observed} - \text{predicted}}{\text{std. error}} = \frac{y_i - \hat{\lambda}_i}{\sqrt{\hat{\lambda}_i}}
$$

$$
(\text{Pearson}) = \sum_{i=1}^{n}(\text{Pearson residual})_i^2
$$

$$
\text{Pearson} \sim \chi^2_{df(n-p)}
$$

-   If ratio is 1, good model fit (no overdispersion)

## Why overdispersion matters

-   If there is overdispersion, then there is more variation in the response than what's implied by a Poisson model. This means:

`r emoji("x")` The standard errors of the model coefficients are artificially small

`r emoji("x")` The p-values are artificially small

`r emoji("x")` We are not fitting the correct model

## Why overdispersion matters

-   We can take overdispersion into account by:

    -   Robust standard errors

    -   **Using a negative-binomial regression model**

# Negative binomial regression model

## Negative binomial regression model

$$
f(y) = \binom{y + \phi - 1}{y} \left ( \frac{\mu}{\mu + \phi} \right )^y \left ( \frac{\phi}{\mu + \phi} \right )^\phi,
$$

$$
Y_i \sim NegBinom(\mu_i, \phi)
$$

-   Basically a poisson model, but allowing for a dispersion parameter $\phi$

$$Var(Y) = \mu + \frac{\mu^2}{\phi}$$

-   Makes the counts more dispersed than with a single parameter

## Running negative binomial

-   `glmer.nb` (nested/multilevel data)

```{r}
#use to run neg binomial
#lme4
m.nb_c <- lme4::glmer.nb(Number_of_fixations ~ emotion*Group + (1|ID), data=hh_data_contrast, verbose=FALSE) 

```

```{r}
#| echo: false
model_parameters(m.nb_c, conf.int =TRUE, effects="fixed") %>%
  print_md()
```

## Running negative binomial

-   `glm.nb` from `MASS` package (non-nested/between-subjects)

```{r eval=T}
#use to run neg binomial
# mass package
m.nb <- MASS::glm.nb(Number_of_fixations ~ emotion*Group, data=hh_data)
```

## Visualize negative binomial

-   Same as Poisson (show expected counts)

```{r, fig.align='center', out.width="60%", echo=T}
plot(ggemmeans(m.nb, terms=c("emotion", 'Group')), show_data=TRUE, jitter=.2)
```

## Model comparison: Poisson or Neg Binom?

-   Can test poisson vs. negative binomial with LRT because they are nested

```{r}
test_likelihoodratio(model2, m.nb_c) %>%
  kable()
```

## Effect sizes

-   Psuedo-$R^2$

```{r}
performance::r2(model1_cont) # glmer
partR2::partR2(model1_cont,data=hh_data, 
  partvars = c("Group", "emotion"),
  R2_type = "marginal", nboot = 10, CI = 0.95
)
```

## Effect sizes

-   Simpler models

    -   Pseudo- $R^2$ (overall model)

```{r}
#| eval: false
    performance::r2_mcfadden(model1) # glm
```

    -   IRR (individual effect)

    -   Cohen's *d (*individual effects)

        -   [RecountD](https://stefany.shinyapps.io/RcountD/)

## Reporting a Poisson/neg binom regression

-   State your hypothesis, statistical test, its link function, and justify the use of Poisson/neg binomial

-   In text, report mean counts/marginal effects

    -   Report log $\lambda$, SE, 95 CIs, p-values, IRR, effect size

::: callout-important
```{r}
#| echo: false
#| 
report(model2)
```
:::

# Zero-inflated Models

## Zero-inflation

-   Too many zeros can bias your results

    -   Overdispersion

    -   Problematic for negative binomial model as well

        -   Assumes 0s are part of the same process (but might not be)

```{r}
#| message: true
#| 
performance::check_zeroinflation(model2)
```

-   Observed \> predicted (zero inflation)

## ZI Models

-   Two parts

    -   Count model

        -   True zeros (sampling 0s)

            -   Poisson or negative binomial

    -   Zeros model

        -   Excess zeros (structural 0s)

            -   Uses a binomial logistic model

                -   0 vs. not 0

## ZI Model

-   `pscl` package `zeroinfl`

-   `glmmTB` package for multilevel zero-inflated models

```{r}
# pscl 
# neg binom model 
zif_model <- zeroinfl(Number_of_fixations ~ emotion*Group, dist=c("negbin"), link=c("log"), data=hh_data_contrast) 
```

```{r}
# glmmTB
model_zip_glmer <- glmmTMB(Number_of_fixations ~ emotion*Group +(1|ID), 
                               data = hh_data_contrast,
                               ziformula = ~1, #estimate zero inflated # add variables here
                               family = nbinom2)
```

## ZI model

```{r}
# pscl
model_parameters(
  model_zip_glmer) %>%
  print_md()
```

# Fin
