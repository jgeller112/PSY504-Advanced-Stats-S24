---
title: "Poisson (üêü), Negative Binomial, and Zero-Inflated Models"
subtitle: "Princeton University"
author: "Jason Geller, PH.D.(he/him)"
date: 'Updated:`r Sys.Date()`'
footer: "PSY 504: Advanced Statistics"
format: 
  revealjs:
    theme: blood
    css: slide-style.css
    multiplex: true
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    fontsize: "25pt"
webr:
  packages: ["tidyverse", "easystats", "broom", "knitr", "emmeans", "ggeffects", "lme4", "pscl", "glmmTMB"]
filters:
  - webr
execute:
  freeze: auto
  echo: true
  message: false
  warning: false
  fig-align: center
  fig-width: 12
  fig-height: 8
  editor_options: 
  chunk_output_type: inline
  code-overflow: wrap
  html:
    code-fold: true
    code-tools: true
---

## Packages

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(emoji)
library(easystats)
library(kableExtra)
library(ggeffects)
library(lme4)
library(DHARMa)
library(knitr)
library(patchwork)
library(viridis)
library(gridExtra)
library(pscl) # zip 
library(glmmTMB) # zero inflated glmer 
library(countreg)

```

- You can follow along here: https://github.com/jgeller112/PSY504-Advanced-Stats-S24/blob/main/slides/07-Poisson/07-Poisson.qmd
## The count

-   Today's lecture brought to you by the number 8

```{r, echo=F, out.width="100%", fig.align='center'}

knitr::include_graphics("images/count.jpeg")
```

## Poisson distribution {data-link="Poisson distribution"}

```{r}
#| echo: false
#| fig-align: "center"
#| 

set.seed(2000)
sim1 <- rpois(100000,1)
sim2 <- rpois(100000,5)
sim3 <- rpois(100000,50)
pois_sim <- tibble (
  sim1 = sim1, 
  sim2 = sim2, 
  sim3 = sim3
)

ggplot(data = pois_sim, aes(x = sim1)) +
  geom_histogram() +
  labs(x = "", title = "lambda:1") + 
  theme_lucid(base_size=25)


```

## Poisson distribution

$$Y_i \sim Poisson(\lambda)$$

Let $Y$ be the number of events in a given unit of time or space.

$$P(Y=y) = \frac{e^{-\lambda}\lambda^y}{y!} \hspace{10mm} y=0,1,2,\ldots, \infty$$

-   $E(Y) = Var(Y) = \lambda$ (*just the mean number of events*)
-   The distribution is typically skewed right, particularly if $\lambda$ is small
-   The distribution becomes more symmetric as $\lambda$ increases
    -   If $\lambda$ is sufficiently large, it can be approximated using a normal distribution

## Poisson distribution

```{r}
#| fig-align: "center"
#| echo: false
#| 
set.seed(2000)
sim1 <- rpois(100000,1)
sim2 <- rpois(100000,5)
sim3 <- rpois(100000,50)
pois_sim <- tibble (
  sim1 = sim1, 
  sim2 = sim2, 
  sim3 = sim3
)
p1 <- ggplot(data = pois_sim, aes(x = sim1)) +
  geom_histogram() +
  labs(x = "", title = "lambda:1")
p2 <- ggplot(data = pois_sim, aes(x = sim2)) +
  geom_histogram() +
  labs(x = "", title = "lambda:5")
p3 <- ggplot(data = pois_sim, aes(x = sim3)) +
  geom_histogram() +
  labs(x = "", title = "lambda:50")
p1 + p2 + p3 
```

```{r echo = F}
sum1 <- c(mean(sim1), var(sim1))
sum2 <- c(mean(sim2), var(sim2))
sum3 <- c(mean(sim3), var(sim3))
data <- rbind(sum1,sum2,sum3)
rownames(data) <- c("lambda = 1", "lambda = 5","lambda = 50")
colnames(data) <- c("Mean", "Variance")
kable(data,format="html")
```

## Examples

The annual number of earthquakes registering at least 2.5 on the Richter Scale and having an epicenter within 40 miles of downtown Memphis follows a Poisson distribution with mean 6.5. **What is the probability there will be 3 or fewer such earthquakes next year?**

$$P(Y<=y) = \frac{e^{-6.5}6.5^0} {0!}+
\frac{e^{-6.5}6.5^1} {1!} + 
 \frac{e^{-6.5}6.5^2} {2!} + 
  \frac{e^{-6.5}6.5^3} {3!}$$

```{webr-r}
a=(exp(-6.5) * 6.5^0) / factorial(0)
b=(exp(-6.5) * 6.5^1) / factorial(1)
c=(exp(-6.5) * 6.5^2) / factorial(2)
d=(exp(-6.5) * 6.5^3) / factorial(3)

ppois(3, 6.5)
```

## Examples

-   Exact count

    -   Let's say you read, on average, 10 pages an hour. **What is the probability you will read 8 pages in an hour?**

. . .

$$P(Y=y)= \frac{e^{-10}10^8} {8!}$$

```{webr-r}

prob <- (exp(-10) * 10^8) / factorial(8)

dpois(x=8, lambda=10)

prob

```

# Poisson regression

## Preferential viewing task

-   The data: viewing behavior to emotional faces

```{r, fig.align='center', echo=FALSE}

include_graphics("images/faces.png")

```

## Preferential viewing task

**Response**:

-   Number of fixations to each face

-   **Predictors**:

    -   `Emotion`: Anger vs. Happy (within-subject)
    -   `Group`: Control vs. Stuttering (between subject)

## The data

```{webr-r}
hh_data <- read.csv("https://raw.githubusercontent.com/jgeller112/psy504-advanced-stats/main/slides/Poisson/data/tobii_aoi_study1.csv")
```

```{r}
#| echo: false
hh_data <- read.csv("https://raw.githubusercontent.com/jgeller112/psy504-advanced-stats/main/slides/Poisson/data/tobii_aoi_study1.csv")
```

## The data

```{r, echo=TRUE}
hh_data <- hh_data %>% dplyr::select(ID, Number_of_fixations, emotion, Group) %>%
  filter(emotion=="Anger"| emotion=="Happy")

head(hh_data) %>%
  kable()
  
```

## Response variable

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| 
ggplot(data = hh_data, aes(x = Number_of_fixations)) +
  geom_histogram() + 
  labs(title = "Total number of fixations")
```
:::

::: {.column width="50%"}
```{r}

hh_data %>% ungroup() %>%
  summarise(mean = mean(Number_of_fixations), var = var(Number_of_fixations), ratio=mean/var) %>%
  kable(digits = 3)


```
:::
:::

## Why the least-squares model doesn't work

The goal is to model $\lambda$, the expected number of fixations on faces, as a function of the predictors (covariates)

. . .

We might be tempted to try a linear model $$\lambda_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{ip}$$

. . .

This model won't work because...

-   It could produce negative values of $\lambda$ for certain values of the predictors

-   The equal variance assumption required to conduct inference for linear regression is violated

## Poisson vs. multiple linear regression

```{r, OLSpois, fig.align="center",out.width="60%", fig.cap='Regression models: Linear regression (left) and Poisson regression (right).',echo=FALSE, warning=FALSE, message=FALSE}
## Sample data for graph of OLS normality assumption
## Code from https://stackoverflow.com/questions/31794876/ggplot2-how-to-curve-small-gaussian-densities-on-a-regression-line?rq=1
set.seed(0)
dat <- data.frame(x=(x=runif(10000, 0, 50)),
                  y=rnorm(10000, 10*x, 100))
## breaks: where you want to compute densities
breaks <- seq(0, max(dat$x), len=5)
dat$section <- cut(dat$x, breaks)
## Get the residuals
dat$res <- residuals(lm(y ~ x, data=dat))
## Compute densities for each section, flip the axes, add means 
## of sections.  Note: densities need to be scaled in relation 
## to section size (2000 here)
dens <- do.call(rbind, lapply(split(dat, dat$section), function(x) {
  d <- density(x$res, n=5000)
  res <- data.frame(x=max(x$x)- d$y*1000, y=d$x+mean(x$y))
  res <- res[order(res$y), ]
  ## Get some data for normal lines as well
  xs <- seq(min(x$res), max(x$res), len=5000)
  res <- rbind(res, data.frame(y=xs + mean(x$y),
                x=max(x$x) - 1000*dnorm(xs, 0, sd(x$res))))
  res$type <- rep(c("empirical", "normal"), each=5000)
  res
}))
dens$section <- rep(levels(dat$section), each=10000)
ols_assume <- ggplot(dat, aes(x, y)) +
  geom_point(size = 0.1, alpha = .25) +
  geom_smooth(method="lm", fill=NA, lwd=2) +
  geom_path(data=dens[dens$type=="normal",], 
            aes(x, y, group=section), 
            color="salmon", lwd=1.1) +
  theme_bw() +
  geom_vline(xintercept=breaks, lty=2)
# Now make Poisson regression picture
set.seed(0)
dat <- data.frame(x=(x=runif(1000, 0, 20)),
                  y=rpois(1000, exp(.1*x)))
## breaks: where you want to compute densities
breaks <- seq(2, max(dat$x), len=5)
dat$section <- cut(dat$x, breaks)
## Get the residuals
dat$res <- dat$y - .1*dat$x
## Compute densities for each section, flip the axes, add means
## of sections.  Note: densities need to be scaled in relation 
## to section size
dens <- do.call(rbind, lapply(split(dat, dat$section), function(x) {
  d <- density(x$res, n=500)
  res <- data.frame(x=max(x$x)- d$y*10, y=d$x+mean(x$y))
  res <- res[order(res$y), ]
  ## Get some data for poisson lines as well
  xs <- seq(min(x$y), max(x$y), len=500)
  res <- rbind(res, data.frame(y=xs,
          x=max(x$x) - 10*dpois(round(xs), exp(.1*max(x$x)))))
  res$type <- rep(c("empirical", "poisson"), each=500)
  res
}))
dens$section <- rep(levels(dat$section), each=1000)
pois_assume <- ggplot(dat, aes(x, jitter(y, .25))) +
  geom_point(size = 0.1) +
  geom_smooth(method="loess", fill=NA, lwd=2) +
  geom_path(data=dens[dens$type=="poisson",], 
            aes(x, y, group=section), 
            color="salmon", lwd=1.1) +
  theme_bw() + ylab("y") + xlab("x") +
  geom_vline(xintercept=breaks, lty=2)
grid.arrange(ols_assume, pois_assume, ncol = 2)
```

.footnote\[From [BMLR Figure 4.1](https://bookdown.org/roback/bookdown-BeyondMLR/ch-poissonreg.html#a-graphical-look-at-poisson-regression)\]

## Poisson regression model

If $Y_i \sim Poisson$ with $\lambda = \lambda_i$ for the given values $x_{i1}, \ldots, x_{ip}$, then

\
$$\log(\lambda_i) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}$$

. . .

-   Each observation can have a different value of $\lambda$ based on its value of the predictors $x_1, \ldots, x_p$

-   $\lambda$ determines the mean and variance, so we don't need to estimate a separate error term

## Assumptions for Poisson regression

-   **Poisson response**: The response variable is a count per unit of time or space, described by a Poisson distribution, at each level of the predictor(s)

-   **Independence**: The observations must be independent of one another

-   **Linearity**: The log of the mean rate, $\log(\lambda)$, must be a linear function of the predictor(s)

-   **Mean = Variance**: The mean must equal the variance

# Poisson regression: Fitting and Interpretation

## Poisson regression: Fitting and Interpretation

```{r, echo=FALSE}
# contrast coding
hh_data_contrast <- hh_data %>%
  mutate(emotion=ifelse(emotion=="Anger", 0.5, -0.5), Group=ifelse(Group=="S", 0.5, -0.5))
```

-   `glm`

```{r, eval=TRUE}

# regular glm 
model_glm <- glm(Number_of_fixations ~ emotion*Group, data = hh_data, family = poisson(link = "log")) # change family to poisson

```

-   `glmer`

```{r}
# fit poisson model# change family to poisson
# repeated measures poisson
#dummy coded
model1 <- glmer(Number_of_fixations ~ emotion+ Group + (1|ID), data = hh_data, family = poisson)
#contrast coded (0.5, -0.5)
model1_cont <- glmer(Number_of_fixations ~ emotion+ Group + (1|ID), data = hh_data_contrast, family = poisson(link = "log")) # change family to poisson
```

## Poisson regression: Fitting and interpretation

-   Coefficients represent log lambda

```{r}
parameters::model_parameters(model1_cont, exponentiate = FALSE, effects="fixed") %>% kable(digits = 3, format = "markdown")
```

## Poisson regression: Fitting and interpretation

-   Mean counts = more interpretable

```{r, fig.align='center', echo=FALSE}

knitr::include_graphics("images/log_poi.png")

```

## Poisson regression: Fitting and interpretation

-   Incidence rate ratios (IRR)

    -   The IRR for a one-unit change in $x_i$ is exp $(\beta)$

    -   The coefficient tells you how changes in X affect the rate at which Y occurs

::: callout-note
IRR \> 1: Expected \# events increases for 1 unit increase

IRR \< 1: Expected \# events decreases for 1 unit increase

IRR = 1: No difference in expected number of events
:::

## Poisson regression: Fitting and interpretation

```{r, echo=FALSE}
model_parameters(model1_cont, exponentiate = FALSE, effects="fixed") %>%
  knitr::kable(digits = 3, format = "markdown")
```

-   exp($\alpha$) = Overall mean count

```{webr-r}

```

## Poisson regression: Fitting and interpretation

::: panel-tabset
## Log

```{r}
model_parameters(model1_cont, effects="fixed") %>% 
  knitr::kable(digits = 3, format = "markdown") %>% kableExtra::row_spec(2, bold= T, color="red", background = "white")
```

## Exp

```{r}
model_parameters(model1_cont, exponentiate = TRUE, effects="fixed") %>% 
  knitr::kable(digits = 3, format = "markdown") %>% kableExtra::row_spec(2, bold= T, color="red", background = "white")
```
:::

::: callout-important
-.08 = Angry faces have -.08 fewer log fixations than happy faces, 
- or exp(-.08) = `r round(exp(-.08), 3)`x that of happy faces - The expected number of fixations is 8% lower for angry faces vs. happy faces
:::

## Main effect: `emotion`

-   Mean counts for each condition

```{r}
marginaleffects::marginal_means(model1, "emotion") %>% 
  kable(digits = 3, format = "markdown")
```

## Marginal Effects

-   Difference in counts

```{r}

marginaleffects::avg_comparisons(model1, variables = "emotion") %>%
  kable()

```

::: callout-important
Happy faces had .33 more fixations than Angry faces
:::

## Poisson regression: Fitting and interpretation

::: panel-tabset
## Log

```{r}
model_parameters(model1_cont, effects="fixed") %>% 
  knitr::kable(digits = 3, format = "markdown") %>% kableExtra::row_spec(., row=3, bold= T, color="red", background = "white")

```

## Exp

```{r}
model_parameters(model1_cont, exponentiate = TRUE, effects="fixed") %>% 
  knitr::kable(digits = 3, format = "markdown") %>% kableExtra::row_spec(., row=3, bold= T, color="red", background = "white")

```
:::

::: callout-important
-   .085 = Stuttering group has .085 more log fixations than Control group, or

    -   exp(0.85) = `r round(exp(.085), 3)`x that of Control group

-   The expected number of fixations is 9% higher for Control  vs. stuttering
:::

## Main effect: `Group`

-   Marginal mean counts

```{r}
marginaleffects::marginal_means(model1, "Group") %>% 
  kable(digits = 3, format = "markdown")

```

## Marginal Effects

-   Difference in counts

```{r}

marginaleffects::avg_comparisons(model1, variables = "Group") %>%
  kable()
```

::: callout-important
Stuttering group had .33 more fixations than Control group.
:::


## Full model

-   LRT test for more complex models

```{r}
#overall model
mod_1 <- car::Anova(model1, type="II")

mod_1 %>% kable(digits = 3, format = "markdown")
```


## Model 2: Add interaction

```{r}
model2 <- glmer(Number_of_fixations ~ emotion*Group+ (1|ID), data = hh_data_contrast, family = poisson)

model_parameters(model2, effects="fixed") %>% 
    kable(digits = 3, format = "markdown")

```

## Add `emotion*Group` to the model?

- `r emoji("check")` Conduct a drop-in-deviance LR test

```{r}
anova(model1_cont, model2, test="chisq") %>% 
  kable()
```

. . .

-   `r emoji("check")` Yes!

## Interaction: `Group*emotion`

-   Simple effects test done on the *response* (count)

```{r}

marginaleffects::avg_comparisons(
    model_glm,
    variables  = "Group", 
    by="emotion") %>%
  kable()

```

. . .

-   More fixations to Angry faces vs. Happy faces for stuttering group

## Visualizing poisson regression

- Used expected/predicted values

```{r, fig.align='center', out.width="60%"}
ggemmeans(model2, terms=c("emotion", "Group")) %>%
  plot(show_data=TRUE, jitter=.1)
```

## Assumptions

```{r, fig.align="center", out.width="60%"}
performance::check_model(model2, check = c("pp_check", "outliers", "overdispersion", "vif"))
```

## Overdispersion

**Overdispersion**: There is more variability in the response than what is implied by the Poisson model

```{r echo = F}
hh_data %>%
  summarise(mean = mean(Number_of_fixations), var = var(Number_of_fixations)) %>%
  kable(digits = 3)
```

```{r echo = F}
hh_data %>%
  group_by(emotion) %>%
  summarise(mean = mean(Number_of_fixations), var = var(Number_of_fixations), ratio=mean/var) %>%
  kable(digits = 3)

```

```{r, echo=F}

hh_data %>%
  group_by(Group) %>%
  summarise(mean = mean(Number_of_fixations), var = var(Number_of_fixations), ratio=mean/var) %>%
  kable(digits = 3)

```

## Testing for overdispersion

-   `Easystats`

```{r}
#| message: true
#| 
check_overdispersion(model2)

```

## Testing for overdispersion

-  Rootograms

  - Expected counts (red lines)
  - Observed counts (bars)
    - Want bars at reference line
      - Overfitting (above 0)
      - Underfitting (below 0)


## Why overdispersion matters

-   If there is overdispersion, then there is more variation in the response than what's implied by a Poisson model. This means:

`r emoji("x")` The standard errors of the model coefficients are artificially small

`r emoji("x")` The p-values are artificially small

`r emoji("x")` This could lead to models that are more complex than what is needed

## Why overdispersion matters

-   We can take overdispersion into account by:

    - Robust standard errors

    - **Using a negative-binomial regression model**

# Negative binomial regression model

## Negative binomial regression model

-   Another approach to handle overdispersion is to use a **negative binomial regression model**

-   Basically a poisson model, but allowing for a dispersion parameter *r*

$$Var(Y) = \mu + \frac{\mu^2}{r}$$

-   Makes the counts more dispersed than with a single parameter

## Running negative binomial

-   `glmer.nb` (nested/multilevel data)

```{r}
#use to run neg binomial
m.nb_c <- glmer.nb(Number_of_fixations ~ emotion*Group + (1|ID), data=hh_data_contrast, verbose=FALSE)

```

```{r}
#| echo: false
model_parameters(m.nb_c, conf.int =TRUE, effects="fixed") %>%
  kable(digits = 3, format = "markdown")
```

## Running negative binomial

-   `glm.nb` (non-nested/between-subjects)

```{r eval=T}
#use to run neg binomial
m.nb <- MASS::glm.nb(Number_of_fixations ~ emotion*Group, data=hh_data)
```

## Visualize negative binomial

-   Same as Poisson (show expected counts)

```{r, fig.align='center', out.width="60%", echo=F}
plot(ggemmeans(m.nb, terms=c("emotion", 'Group')), show_data=TRUE, jitter=.1) +
  theme_lucid(base_size = 25)
```

## Effect sizes

-   Psuedo-$R^2$

```{r}

performance::r2(model1_cont) # glmer
#performance::r2_mcfadden(model1) # glm

partR2::partR2(model1_cont,data=hh_data, 
  partvars = c("Group", "emotion"),
  R2_type = "marginal", nboot = 10, CI = 0.95
)

```

## Effect sizes

-   [RecountD](https://stefany.shinyapps.io/RcountD/)

    -  Cohen's *d*

## Reporting a poisson regression

-   State your hypothesis, statistical test, its link function, and justify the use of a poisson regression

    -   We hypothesized that years that had more water main breaks would have fewer renewable projects approved. The number of renewable projects approved by the City of Toronto represented a count of rare events, which violated the normality assumption required for traditional regression. Thus, a poisson regression with a log link function was used to predict the number of renewable projects in Toronto in a given year using R 4.0.4 (R Core Team, 2020). Prior to the analysis, the number of water main breaks was mean centered. Furthermore, the number of water main breaks was divided by 100 to improve interpretation of the slopes. Effect sizes that approximate Cohen‚Äôs d were calculated using the RCountD Shiny App (Coxe, 2018).

## Reporting a Poisson Regression

-   State the full model and your results

    -   The number of renewable projects in a given year were modeled as a function of water main breaks in the same year. As shown in Figure 1, this analysis revealed that years with more water main breaks had fewer renewable projects approved, b = -0.15, SE = 0.06, z(11) = -2.56, p = 0.01, d = -0.27

## Zero-inflation

-   Too many zeros can bias your results

    -   Overdispersion

      - Problematic for negative binomial model as well

        -  Assumes 0s are part of the same process (but might not be)

```{r}
#| message: true
#| 
performance::check_zeroinflation(model1_cont)
```

- Observed > predicted (zero inflation)

# Zero-inflated Models

## ZIP Models

-   Two parts

    -   Count model

        -   True zeros (sampling 0s)

            -  Poisson or negative binomial

    -   Zeros model

        -   Excess zeros (structural 0s)

            -   Uses a binomial logistic model

                -   0 vs. not 0

## ZIP Model

-   `pscl` package `zeroinfl`

-   `glmmTB` package for multilevel zero-inflated models

```{r}
# pscl 
# neg binom model 
zif_model <- zeroinfl(Number_of_fixations ~ emotion*Group, dist=c("negbin"), link=c("log"), data=hh_data_contrast) 
```

```{r}
# glmmTB
model_zip_glmer <- glmmTMB(Number_of_fixations ~ emotion*Group +(1|ID), 
                               data = hh_data_contrast,
                               ziformula = ~1, #estimate zero inflated
                               family = nbinom2)
```

## ZIP model

```{r}
# pscl
model_parameters(
  model_zip_glmer)
```


## Model comparison: Poisson or Neg Binom?

- Can test poisson vs. negative binomial with LRT because they are nested

```{r}
test_likelihoodratio(model2, m.nb_c) %>%
  kable()
```

