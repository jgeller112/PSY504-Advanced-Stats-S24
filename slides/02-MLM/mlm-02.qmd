---
title: "Multilevel Modeing (with R) Part 2"
subtitle: "Princeton University"
author: "Jason Geller, PH.D.(he/him)"
date: 'Updated:`r Sys.Date()`'
footer: "PSY 504: Advaced Statistics"
format: 
  revealjs:
    theme: blood	
    css: slide-style.css
    multiplex: true
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    fontsize: "25pt"
    highlight-style: github-dark
webr:
  packages: ["tidyverse", "easystats", "broom", "kableExtra", "interactions", "emmeans", "lme4","lmertest",  "ggeffects"]
filters:
  - webr
execute:
  freeze: auto
  echo: true
  message: false
  warning: false
  fig-align: center
  fig-width: 16
  fig-height: 12
  editor_options: 
  chunk_output_type: inline
  code-overflow: wrap
  html:
    code-fold: true
    code-tools: true
---

## Multilevel models

-   When to use them:

    -   Nested designs

    -   Repeated measures

    -   Longitudinal data

    -   Complex designs

-   Why use them:

    -   Captures variance occurring between groups and within groups

-   What they are:

    -   Linear model with extra residuals

## Today

-   Everything you need to know to run and report a MLM

    -   Organizing data for MLM analysis
    -   Estimation
    -   Fit and interpret multilevel models
    -   Visualizion
    -   Effect size
    -   Reporting
    -   Power

## Packages

```{r}
library(tidyverse) # data wrangling
library(knitr) # nice tables
library(lme4) # fit mixed models
library(lmerTest) # mixed models
library(broom.mixed) # tidy output of mixed models
library(afex) # fit mixed models for lrt test
library(emmeans) # marginal means
library(ggeffects) # marginal means
library(ggrain) # rain plots
library(easystats) # nice ecosystem of packages

options(scipen=999) # get rid of sci notation

```

## Today's data

-   What did you say?

    -   Ps (*N* = 31) listened to *both* clear (NS) and 6 channel vocoded speech (V6)
        -   (https://www.mrc-cbu.cam.ac.uk/personal/matt.davis/vocode/a1_6.wav)
            -   Fixed factor: ?
            -   Random factor: ?

## Today's data

```{r}
eye  <- read_csv("https://raw.githubusercontent.com/jgeller112/psy504-advanced-stats/main/slides/MLM/data/vocoded_pupil.csv") # data for class

```

```{r}
#| echo: false
#| fig-align: "center"
#| 

eye %>% group_by(subject, vocoded) %>% summarise(mean_pupil=mean(mean_pupil)) %>% 
  ggplot(., aes(vocoded, mean_pupil, fill = vocoded)) +
  geom_rain(alpha = .5, id.long.var = "subject") +
  scale_fill_manual(values=c("dodgerblue", "darkorange")) +
  guides(fill = 'none', color = 'none') + 
  labs(y="Mean Pupil Size") + 
    theme_lucid(base_size=18)

```

## Data organization

-   Data Structure

    -   MLM analysis (in R) requires data in long format

![](images/pivot_longer.png){fig-align="center"}

## Data organization

-   Level 1: trial

-   Level 2: subject

```{r}
#| echo: false
head(eye) %>%
  select(-...1) %>% 
  kable()
```

## Centering

::: columns
::: {.column width="50%"}
-   In a single-level regression, centering ensures that the zero value for each predictor is meaningful before running the model

-   In MLM, if you have specific questions about within, between, and contextual effects, you need to center!
:::

::: {.column width="50%"}
![](images/centering_mlm.png)

![](){fig-align="center"}
:::
:::

## Group- vs. Grand-Mean Centering

-   Grand-mean centering: $x_{ij} - x$

    -   Variable represents each observation's deviation from everyone's norm, regardless of group

-   Group-mean centering: $x_{ij} - x_j$

    -   Variable represents each observation's deviation from their group's norm (removes group effect)

## Group- vs. Grand-Mean Centering

-   Level 1 predictors

    -   Grand-mean centering

        -   **Include means of level 2**
            -   Allows us to directly test within-group effect
            -   Coefficient associated with the Level 2 group mean represents **contextual effect**

    -   Group-mean centering

        -   Level 1 coefficient will always be with within-group effect, regardless of whether the group means are included at Level 2 or not
        -   If level 2 means included, coefficient represents the between-groups effect

::: callout-note
Can apply to categorical predictors as well (see Yaremych, Preacher, & Hedeker, 2023)
:::

## Centering in R

```{r, eval=FALSE}

library(datawizard) #easystats 

# how to group mean center 
d <- d %>% 
  # Grand mean centering (CMC)
  mutate(iv.gmc = iv-mean(iv)) %>%
  # group  mean centering (more generally, centering within cluster)
  group_by(id) %>% 
  mutate(iv.cm = mean(iv),
         iv.cwc = iv-iv.cm)

#data wizard way
x <- demean(x, select=c("x"), group="ID") #gets within-group and between group clusters

```

# Model Estimation

## Maximum Likelihood

<br>

<br>

<br>

-   In MLM we try to maximize the likelihood of the data

    -   No OLS!

## Probability vs. Likelihood

-   Probability

> If I assume a distribution with certain parameters (fixed), what is the probability I see a particular value in the data?

::: columns
::: {.column width="50%"}
-   Pr‚Å°(ùë¶\>0‚îÇùúá=0,ùúé=1)=.50

-   Pr‚Å°(‚àí1\<ùë¶\<1‚îÇùúá=0,ùúé=1)=.68

-   Pr‚Å°(0\<ùë¶\<1‚îÇùúá=0,ùúé=1)=.34

-   Pr‚Å°(ùë¶\>2‚îÇùúá=0,ùúé=1)=.02
:::

::: {.column width="50%"}
![](images/normal.png){fig-align="center"}
:::
:::

## Likelihood

::: columns
::: {.column width="50%"}
-   $L(ùúá,ùúé‚îÇùë•)$

-   Holding a sample of data constant, which parameter values are more likely?

    -   Which values have higher likelihood?

    *Here data is fixed and distribution can change*
:::

::: {.column width="50%"}
![](images/likelihood-2.png){fig-align="center"}
:::
:::

## Likelihood

```{r, echo=FALSE, fig.align='center', out.width="100%"}

knitr::include_graphics("images/like1.png")

```

## Likelihood

```{r, echo=FALSE, fig.align='center', out.width="100%"}

knitr::include_graphics("images/like2.png")

```

## Likelihood

```{r, echo=FALSE, fig.align='center', out.width="100%"}

knitr::include_graphics("images/like4.png")

```

## Likelihood

```{r, echo=FALSE, fig.align='center', out.width="100%"}

knitr::include_graphics("images/like5.png")

```

## Likelihood

```{r, echo=FALSE, fig.align='center', out.width="100%"}

knitr::include_graphics("images/like6.png")
```

## Likelihood

Interactive: Understanding Maximum Likelihood Estimation: <https://rpsychologist.com/likelihood/>

## Log likelihood

-   With large samples, likelihood values ‚Ñí(ùúá,ùúé‚îÇùë•) get very small very fast

    -   To make them easier to work with, we usually work with the log-likelihood
        -   Measure of how well the model fits the data
        -   Higher values of $\log L$ are better

-   Deviance = $-2logL$

    -   $-2logL$ follows a $\chi^2$ distribution with $n (\text{sample size}) - p (\text{paramters}) - 1$ degrees of freedom

## $\chi^2$ distribution

```{r, echo=F, fig.height = 6, fig.align='center'}
x <- seq(from =0, to = 10, length = 100)
# Evaluate the densities
y_1 <- dchisq(x, 1)
y_2 <- dchisq(x,2)
y_3 <- dchisq(x,3)
y_4 <- dchisq(x,5)
# Plot the densities
plot(x, y_1, col = 1, type = "l", ylab="",lwd=3, ylim = c(0, 0.5), 
     main  = "Chi-square Distribution")
lines(x,y_2, col = 2,lwd=3)
lines(x, y_3, col = 3,lwd=3)
lines(x, y_4, col = 4,lwd=3)
# Add the legend
legend("topright",
       c("df = 1", "df = 2 ", "df = 3", "df = 5"), 
       col = c(1, 2, 3, 4), lty = 1)
```

## Comparing nested models

-   Suppose there are two models:

    -   Reduced model includes predictors $x_1, \ldots, x_q$
    -   Full model includes predictors $x_1, \ldots, x_q, x_{q+1}, \ldots, x_p$

-   We want to test the hypotheses:

    -   $H_0$: smaller model is better

    -   $H_1$: Larger model is better

-   To do so, we will use the drop-in-deviance test (also known as the nested likelihood ratio test)

## Drop-In-Deviance Test

-   Hypotheses:

    -   $H_0$: smaller model is better

    -   $H_1$: Larger model is better

-   Test Statistic: $$G = (-2 \log L_{reduced}) - (-2 \log L_{full})$$

-   P-value: $P(\chi^2 > G)$:

    -   Calculated using a $\chi^2$ distribution
    -   df = $df_1$ - $df_2$

## Testing deviance

-   We can use the `anova` function to conduct this test

    -   Add test = "Chisq" to conduct the drop-in-deviance test

-   I like `test_likelihoodratio` from `easystats`

```{r, eval=FALSE}
anova(model1, model2, test="chisq")

# test using easystats function

test_likelihoodratio(model1, model2)

```

## Model fitting: ML or REML?

-   Two flavors of maximum likelihood

    -   Maximum Likelihood (ML or FIML)

        -   Jointly estimate the fixed effects and variance components using all the sample data

        -   Can be used to draw conclusions about fixed and random effects

        -   Issue:

            -   Results are biased because fixed effects are estimated without error

## Model fitting: ML or REML

-   Restricted Maximum Likelihood (REML)

    -   Estimates the variance components using the sample residuals not the sample data

    -   It is conditional on the fixed effects, so it accounts for uncertainty in fixed effects estimates

        -   This results in unbiased estimates of variance components
        -   Associated with error/penalty

## Model fitting: ML or REML?

-   Research has not determined one method absolutely superior to the other

-   **REML** (`REML = TRUE`; default in `lmer`) is preferable when:

    -   The number of parameters is large

    -   Primary objective is to obtain relaible estimates of the variance parameters

    -   For REML, likelihood ratio tests can only be used to draw conclusions about variance components

-   **ML** (`REML = FALSE`) <u>must</u> be used if you want to compare nested fixed effects models using a likelihood ratio test (e.g., a drop-in-deviance test)

## ML or REML?

-   What would we use if we wanted to compare the below models?

```{r eval=FALSE}

x= lmer(DV ~ IV1 + IV2 + (1|ID))

y= lmer(DV ~ IV1*IV2 + (1|ID))

```

## ML or REML?

-   What would we use if we wanted to compare the below models?

```{r eval=FALSE}

x = lmer(DV ~ IV1 + IV2 + (1+IV2|ID))

y = lmer(DV ~ IV1+ IV2 + (1|ID))

```

# Fitting and Interpreting Models

## Modeling approach

-   Forward approach

```{webr-r}

```

-   `Keep it maximal`[^1]

    -   Whatever can vary, should vary

        -   **Decreases Type 1 error**

[^1]: Barr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of memory and language, 68(3), 10.1016/j.jml.2012.11.001. https://doi.org/10.1016/j.jml.2012.11.001

## Modeling approach

-   Full (maximal) model

    -   Only when there is convergence issues should you remove terms
        -   if non-convergence (pay attention to warning messages in summary output!):
            -   Try different optimizer (`afex::all_fit()`)
                -   Sort out random effects
                    -   Remove correlations between slopes and intercepts
                    -   Random slopes
                    -   Random Intercepts
                -   Sort out fixed effects (e.g., interaction)
                -   Once you arrive at the final model present it using REML estimation

## Modeling approach

-   If your model is singular (check output!!!!)

    -   Variance might be close to 0
    -   Perfect correlations (1 or -1)

-   Drop the parameter!

## Modeling approach

```{r}
data <- read.csv("https://raw.githubusercontent.com/jgeller112/PSY504-Advanced-Stats-S24/main/slides/02-MLM/data/heck2011.csv")

summary(lmer(math~ses + (1+ses|schcode), data=data))

```

## Null model (unconditional means)

Get ICC

-   ICC is a standardized way of expressing how much variance is due to clustering/group
    -   Ranges from 0-1
-   Can also be interpreted as correlation among observations within cluster/group!
-   If ICC is sufficiently low (i.e., $\rho$ \< .1), then you don't have to use MLM! *BUT YOU PROBABLY SHOULD üôÇ*

## Null model (unconditional means)

```{r}
library(lme4) # pop linear modeling package

null_model <- lmer(mean_pupil ~ (1|subject), data = eye, REML=TRUE)

summary(null_model)

```

## Calculating ICC

-   Run baseline (null) model

-   Get intercept variance and residual variance

$$\mathrm{ICC}=\frac{\text { between-group variability }}{\text { between-group variability+within-group variability}}$$

$$
ICC=\frac{\operatorname{Var}\left(u_{0 j}\right)}{\operatorname{Var}\left(u_{0 j}\right)+\operatorname{Var}\left(e_{i j}\right)}=\frac{\tau_{00}}{\tau_{00}+\sigma^{2}}
$$

```{r}
# easystats 
icc <- performance::model_performance(null_model)

icc$ICC
```

## Maximal model: Fixed effect random intercepts (subject) and slopes (vocoded) model

```{r}

max_model <- lmer(mean_pupil ~vocoded +(1+vocoded|subject), data = eye)

summary(max_model)
```

## Fixed effects

-   Interpretation same as lm

```{r}
#grab the fixed effects
summary(max_model)
```

## Degrees of freedom and p-values

-   Degrees of freedom (denominator) can be assessed with several methods:

    -   ***Satterthwaite*** (default when install `lmerTest` and then run `lmer`)

    -   Asymptotic (Inf) (**default** behavior lme4)

    -   Kenward-Rogers

## Random effects/variance components

-   Tells us how much variability there is around the fixed intercept/slope

    -   How much does the average pupil size change between participants

```{r}

summary(max_model)

```

## Random effects/variance components

-   Correlation between random intercepts and slopes

    -   Negative correlation

        -   Higher intercept (for normal speech) less of effect (lower slope)

```{r}
#| echo: false
#| fig-align: "center"
#| fig-width: 12
#| fig-height: 8
re = ranef(max_model)$subject # get random effects 

cor_test(re,  "vocodedV6", "(Intercept)") %>% 
  plot()
```

## Visualize Random Effects

```{r}
#| fig-align: center
#| 
# use easystats to grab group variance
random <- estimate_grouplevel(max_model)

plot(random) + theme_lucid()
```

## Model comparisons

-   Can compare models using `anova` function or `test_likelihoodratio` from `easystats`

    -   *Will be refit using ML if interested in fixed effects*

```{r}
# you try
```

# AIC/BIC

-   LRT requires nested models

## AIC

-   AIC:

$$
D + 2p
$$

-   where d = deviance and p = \# of parameters in model

-   Can compare AICs[^2]:

    $$
     \Delta_i = AIC_{i} - AIC_{min}
    $$

-   Less than 2: More parsimonious model is preferred

-   Between 4 and 7: some evidence for lower AIC model

-   Greater than 10,: strong evidence for lower AIC

[^2]: BURNHAM, ANDERSON, & HUYVAERT (2011)

## BIC

-   BIC:

$$
D + ln(n)*p
$$

-   where d = deviance, p = \# of parameters in model, n = sample size

-   Change in BIC:

    -   $\Delta{BIC}$ \<= 2 (No difference)

    -   $\Delta{BIC}$ \> 3 (evidence for smaller BIC model)

## AIC/BIC

```{r}

performance::model_performance(max_model) %>% # easystats
  kable()

```

## Hypothesis testing

-   Multiple options

    1.  t/F tests with approximate degrees of freedom (Kenward-Rogers or Satterwaithe)
    2.  Parametric bootstrap
    3.  **Likelihood ratio test (LRT)**

    -   Can be interpreted as main effects and interactions
    -   Use `afex` package to do that

## Hypothesis testing - `afex`

```{r}
library(afex) # load afex in 

m <- mixed(mean_pupil ~ 1 + vocoded +  (1+vocoded|subject), data =eye, method = "LRT") # fit lmer using afex

nice(m) %>%
  kable()
```

## Using `emmeans`

-   Get means and contrasts

```{r}

library(emmeans) # get marginal means 

emmeans(max_model, specs = "vocoded") %>% 
  kable() # grabs means/SEs for each level of vocode 

pairs(emmeans(max_model, specs = "vocoded")) %>%
  confint() %>%
  kable()
# use this to get pariwise compairsons between levels of factors
```

# Assumptions

## Check assumptions

::: columns
::: {.column width="50%"}
-   Linearity

-   Normality

    -   Level 1 residuals are normally distributed around zero

    -   Level 2 residuals are multivariate-normal with a mean of zero

-   Homoskedacticity

    -   Level 1 residual variance is homoskedastic both within- and between-groups
:::

::: {.column width="50%"}
-   Collinearity

-   Outliers
:::
:::

## Assumptions

```{r}
#| fig-align: "center"
#| 
library(easystats) # performance package

check_model(max_model)
```

## Visualization

```{r}
#| echo: false

pupil_data_mean <- eye %>%
  group_by(subject, vocoded) %>%
  summarise(mean_pup=mean(mean_pupil, na.rm=TRUE)) %>% 
  ungroup()

mod_plot <- max_model %>%
  estimate_means("vocoded") %>%
  as.data.frame()

pupil_plot_lmer <- ggplot(pupil_data_mean, aes(x = vocoded, y = mean_pup)) +     
  geom_violinhalf(aes(fill = vocoded), color = "white") +
  geom_jitter2(width = 0.05, alpha = 0.5, size=5) +  # Add pointrange and line from means
  geom_line(aes(y=mean_pup, group=subject))+
  geom_line(data = mod_plot, aes(y = Mean, group = 1), size = 3, color="purple") +
  geom_pointrange(
    data = mod_plot,
    aes(y = Mean, ymin = CI_low, ymax = CI_high),
    size = 2,
    color = "purple"
  ) + 
  # Improve colors
  scale_fill_material() +
  theme_modern() + 
  ggtitle("Pupil Effect", subtitle = "White dots represent model mean and error bars represent 95% CIs. Black dots are group level means for each person")

pupil_plot_lmer

```

## `ggeffects`

```{r}
#| fig-align: "center"

ggemmeans(max_model, terms=c("vocoded")) %>% plot()
```

## Effect size

-   Report pseudo-$R^2$ for marginal (fixed) and conditional model (random) (Nakagawa et al. 2017)

$$
R^2_{LMM(c)} = \frac{\sigma_f^2\text{fixed} + \sigma_a^2\text{random}}{\sigma_f^2\text{fixed} + \sigma_a^2\text{random} + \sigma_e^2\text{residual}}
$$

$$
R^2_{\text{LMM}(m)} = \frac{\sigma_f^2\text{fixed}}{\sigma_f^2\text{fixed} + \sigma_a^2\text{random} + \sigma_e^2\text{residual}}
$$

-   Report semi-partial $R^2$ for each predictor variable
    -   $R^2_\beta$
        -   `partR2` package in R does this for you

## Effect size

```{r}
#| eval: false
#| 
#get r2 for model with performance fro easystats

performance::r2(max_model) 

# get semi-part
library(partR2)
# does not work with random slopes for some reason :/
R2_3 <- partR2(max_model,data=eye, 
  partvars = c("vocoded"),
  R2_type = "marginal", nboot = 10, CI = 0.95
)
```

## Effect size

-   Cohen's $d$ for treatment effects/categorical predictions[^3]

[^3]: Brysbaert, M., & Debeer, D. (2023, September 12). How to run linear mixed effects analysis for pairwise comparisons? A tutorial and a proposal for the calculation of standardized effect sizes. <https://doi.org/10.31234/osf.io/esnku>

$$
d = \frac{\text{Effect}}{\sqrt{\sigma^2_\text{Intercept} + \sigma^2_\text{slope} + \sigma^2_\text{residual}}}
$$

```{r}

emmeans(max_model,~ vocoded) %>% 
 eff_size(.,sigma=.04, edf=30) # need to calcuate sigma and add dfs

```

# Reporting Results

## Describing a MLM analysis

-   Structure

    -   What was the nested data structure (e.g., how many levels; what were the units at each level?)

        ‚Ä¢ How many units were in each level, on average?

        ‚Ä¢ What was the range of the number of lower-level units in each group/cluster?

## Describing a MLM analysis

::: columns
::: {.column width="50%"}
-   What equation can best represent your model?

-   What estimation method was used (e.g., ML, REML)?

-    If there were convergence issues, how was this addressed?

-   What software (and version) was used (when using R, what packages as well)?
:::

::: {.column width="50%"}
-   If degrees of freedom were used, what kind?

-   What type of models were estimated (i.e., unconditional, random intercept, random slope, max)?

-   What variables were centered and what kind of centering was used?

-   What model assumptions were checked and what were the results?
:::
:::

## Reporting Results

::: columns
::: {.column width="50%"}
-   What was the ICC of the outcome variable?

-   Are fixed effects and variance components reported?

-   What inferential statistics were used (e.g., t-statistics, LRTs)?

-   How precise were the results (report the standard errors and/or confidence
    intervals)?
:::

::: {.column width="50%"}
-   Were model comparisons performed (e.g., AIC, BIC, if using an LRT,report the œá2, degrees of freedom, and p value)?

-   Were effect sizes reported (e.g., Cohen‚Äôs d, pseudo-R2?

-   What kind of pseudo-R2 were reported?
:::
:::

## Write-up

```{r}
#| eval: false
report::report(max_model) # easystats report function
```

`r report(max_model)`

## Table

```{r}
modelsummary::modelsummary(list("max model" = max_model), output="kableExtra") # modelsummary package
```

## Power

-   Simulation-based power analyses

    -   Simulate new data

        -   `faux` (<https://debruine.github.io/faux/articles/sim_mixed.html>)

    -   Use pilot data (what I would do)

        -   `mixedpower`(https://link.springer.com/article/10.3758/s13428-021-01546-0)
        -   `simr` (<https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504>)

    ## 
