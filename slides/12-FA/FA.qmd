---
title: "Introduction to Exploratory and Confirmatory Factor Analysis (Using R)"
subtitle: "Princeton University"
author: "Jason Geller, PH.D."
date: 'Updated:`r Sys.Date()`'
footer: "PSY 504: Advanced Statistics"
format: 
  revealjs:
    theme: style_new.scss
    multiplex: true
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    fontsize: "25pt"
webr:
  packages: ["tidyverse"]
filters:
  - webr
execute:
  freeze: auto
  echo: true
  message: false
  warning: false
  fig-align: center
  fig-width: 12
  fig-height: 8
  editor_options: 
  chunk_output_type: inline
  code-overflow: wrap
  html:
    code-fold: true
    code-tools: true
---

## Today

-   Common factor analysis and PCA
-   Carrying out exploratory factor analysis in R
    -   Steps
-   CFA
-   Reporting factor analysis

## Packages

```{r}

library(tidyverse)
library(easystats)
library(psych) # conduct fa 
library(semPlot) # plot factor analysis
library(factoextra) # factor analysis 
library(corrplot) # correlation viz
library(kableExtra)

options(scipen = 999)
```

-   Follow along here: <https://github.com/jgeller112/PSY504-Advanced-Stats-S24/blob/main/slides/12-FA/FA.qmd>

## What is factor analysis?

-   **Let's say we have 6 items in a scale:**

    -   Sleep disturbances (insomnia/hypersomnia)

    -   Suicidal ideation

    -   Lack of interest in normally engaging activities

    -   Racing thoughts

    -   Constant worrying

    -   Nausea

. . .

-   FA "looks" at the relationships between these items and finds that some of them seem to hang together

## What is factor analysis?

::: columns
::: {.column width="50%"}
-   Let's say we have 6 items in a scale:

    -   **Sleep disturbances (insomnia/hypersomnia)**

    -   **Suicidal ideation**

    -   **Lack of interest in normally engaging activities**

    -   [Racing thoughts]{.underline}

    -   [Constant worrying]{.underline}

    -   [Nausea]{.underline}

        -   Some of these could cross-load

        -   FA considers this and items load on all factors
:::

::: {.column width="50%"}
<br>

<br>

![](images/cfa.png){fig-align="center"}
:::
:::

## Why?

::: columns
<br>

<br>

::: {.column width="50%"}
-   Allows you to summarize complex data with a smaller set of representative variables

    -   6 variables to 2 variables
:::

::: {.column width="50%"}
![](images/IMG_2412.jpeg)
:::
:::

## Why?

::: columns
<br>

<br>

::: {.column width="50%"}
-   Can help identify/confirm underlying constructs

    -   Depression and anxiety
:::

::: {.column width="50%"}
![](images/IMG_2411.jpeg)
:::
:::

## Partitioning Variance

::: columns
::: {.column width="50%"}
1.  Variance common to other variables
    -   Communality $h^2$: **proportion of each variable's/item's variance that can be explained by the factors**
        -   How much an item is related to other items in the analysis
2.  Variance specific to that variable (unique variance)
3.  Random measurement error
:::

::: {.column width="50%"}
![](images/fapcavar.png){fig-align="center"}
:::
:::

## Common factor analysis

::: columns
::: {.column width="50%"}
-   Common factor analysis

    -   Attempts to achieve parsimony (data reduction) by:
        -   Explaining the *maximum amount of common variance* in a correlation matrix
            -   Using the *smallest* number of explanatory constructs (factors)
:::

::: {.column width="50%"}
![](images/IMG_2409.jpeg)
:::
:::

## Common factor analysis

> Partitions variance that is in common with other variables. How?

-   Use multiple regression to calculate multiple $R^2$

    -   Each item as an outcome

    -   Use all other items as predictors

    -   Finds the communality among all of the variables, relative to one another

## Common factor analysis

![](images/IMG_2413.jpeg)

## Common factor analysis

![](images/IMG_2414.jpeg)

## Common factor analysis

![](images/IMG_2415.jpeg)

## PCA

![](images/IMG_2410-01.jpeg){fig-align="center"}

-   Assumption: Components that explain the most total variance

-   Goal: Find fewest components that account for the relationships among variables

## PCA vs. FA

<br>

<br>

-   Run *factor analysis* if you assume or wish to test a theoretical model of *latent factors* causing observed variables

-   Run PCA If you want to simply *reduce* your correlated observed variables to a smaller set of important independent composite variables

## Eigenvalues and Eigenvectors

-   **Eigenvalues** represent the total amount of variance that can be explained by a given factor

    -   Sum of squared component loadings down all items for each factor

-   **Eigenvectors** represent a weight for each eigenvalue

    -   Eigenvector times the square root of the eigenvalue gives the **factor loadings**

        -   Correlation between item and factor

## Exploratory factor analysis steps

1.  Checking the suitability of data (should we run a factor analysis?)

2.  Decide \# of factors

3.  Extraction

4.  Rotation (make factors more interpretable)

5.  Interpret/name

## Big 5

::: columns
::: {.column width="50%"}
![](https://measuringsel.casel.org/wp-content/uploads/2018/08/big5.png){fig-align="center"}
:::

::: {.column width="50%"}
-   2800 participants

-   25 self-report items from big 5 inventory

    -   The personality items are split into 5 categories
:::
:::

## Data

```{r}
#| echo: false
#| # Load the data
data <- read.csv("https://raw.githubusercontent.com/jgeller112/PSY504-Advanced-Stats-S24/main/slides/12-FA/data/bfi.csv")
data <- data[, 2:25] # Select only the 25 first columns corresponding to the items
data <- na.omit(data)
```

```{webr-r}
#| echo: true
#| # Load the data
data <- read.csv("https://raw.githubusercontent.com/jgeller112/PSY504-Advanced-Stats-S24/main/slides/12-FA/data/bfi.csv")
data <- data[, 2:25] # Select only the 25 first columns corresponding to the items
data <- na.omit(data)
```

## Data visualization

```{r}
#| echo: false
#| fig.align: "center"
d=cor(data)

corrplot(d, method = 'square', type = 'lower', diag = FALSE, outline = T, addgrid.col = "darkgray")
```

## Is factor analysis warranted?

-   Bartlett's test

    -   Is the Correlation matrix significantly different from an identity matrix (0s)?

        |     |     |     |
        |-----|-----|-----|
        | 1   | 0   | 0   |
        | 0   | 1   | 0   |
        | 0   | 0   | 1   |

. . .

-   Yes. There are correlations between the variables

-   No. No correlations and factor analysis is not suitable

## Is factor analysis warranted?

-   Kaiser-Meyer-Olkin (KMO)

$$
KMO = \frac{\Sigma(r)^2}{\Sigma(r)^2 + \Sigma(r_p)^2}
$$

-   If variables share a common factor they will have small partial correlation (i.e., most of the variance is explained by common factor so not much left)

    | **KMO Criterion** | **Adequacy Interpretation** |
    |-------------------|-----------------------------|
    | 0.70-0.79         | Good                        |
    | 0.80-0.89         | Very Good                   |
    | 0.90-1.00         | Excellent                   |

## Is factor analysis warranted?

-   Check's Bartlett's
-   Checks KMO
-   Check MSA (should delete MSA \< .5)

```{r}
#easystats
performance::check_factorstructure(data)
```

## Assumptions

-   No outliers

-   Large sample

    -   100

-   Normality

-   No missingness

-   No multicolinerarity

## Assumptions: Outliers

```{r}
performance::check_outliers(data)
outliers_list<- performance::check_outliers(data) 
data <- data[!outliers_list, ] # remove outliers
```

## Assumptions: Multicollinearity

-   We do not want variables that are too highly correlated

-   Determinant of correlation matrix

    -   Smaller \< .00001 (close to 0) suggests a problem with multicollinearity

```{r}
cormatrix <- cor(data)
det(cormatrix)
```

## Fitting factor model: \# of factors

Several different ways:

-   A priori

-   Eigenvalues \> 1 (Kaiser criterion)

-   Cumulative percent variance extracted (75%)

```{r}
    #| eval: False
    #| 
    number_items <- fa.parallel(data, fm="pa", fa="fa")
    #eigenvalues > 1 and > .7
    sum(number_items$fa.values >= 1)


```

## Fitting factor model: \# of factors

<br>

<br>

::: columns
::: {.column width="50%"}
Scree plot

-   A plot of the Eigenvalues in order from largest to smallest

-   Look for the elbow (shared variability starting to level off)

    -   Above the elbow is how many components you want
:::

::: {.column width="50%"}
```{r}
#| echo: FALSE
#| fig.width: 8
#| fig.height: 4
res.pca <- prcomp(data)
fviz_screeplot(res.pca, addlabels=TRUE)
```
:::
:::

## Fitting factor model: \# of factors

::: columns
::: {.column width="50%"}
-   Parallel analysis

    -   More objective and reliable

    -   Run simulations pulling eigenvalues from randomly generated datasets

    -   If eigenvalues \> eigenvalues from random datasets more likely to represent meaningful patterns in the data
:::

::: {.column width="50%"}
```{r}

fa.parallel(data, fm="pa", fa="fa")

```
:::
:::

## Method agreement procedure

-   Uses many methods to determine how many factor you should get

    -   This is the approach I would use

```{r}
#| fig-align: "center"
#| 
parameters::n_factors(data) %>% plot()

```

## Extracting factor loadings

::: columns
::: {.column width="50%"}
-    Runs another factor analysis to get the loading for each of the factors

    -   Principal axis factoring (PAF)

        -   Get initial estimates of communalities

            -   Squared multiple correlations (highest absolute correlation)

        -   Take correlation matrix and replace diagonal elements with communalities
:::

::: {.column width="50%"}
![](images/IMG_2416.jpeg)
:::
:::

## Running factor analysis

```{r}
# nfactor number of factors from par analysis
# rotate rotation method 
# fm is principle axis
efa <- psych::fa(data, nfactors = 5, rotate="none", fm="pa")
```

## Factor loadings

::: panel-tabset
## Factor Loadings

-   Pattern matrix

    -   Correlation between item and factor

```{r}
#| echo: false
parameters::model_parameters(efa) %>% kable() %>% 
  kable_styling(font_size = 24) %>%
 column_spec(2:2, color = "white",
              background = "red")

```

## Naming

-   Naming: PA1-PA2...

    -   Reflects fitting method

```{r}
#| echo: false
#| 
parameters::model_parameters(efa) %>% kable() %>% 
  kable_styling(font_size = 24)

```

## Complexity

-   Complexity

    -   Number of factors an item loads on (ideally 1!)

```{r}
#| echo: false
parameters::model_parameters(efa) %>% kable() %>% 
  kable_styling(font_size = 24) %>%
 column_spec(7:7, color = "white",
              background = "red")
```

## Uniqueness

-   1-commonality

```{r}
#| echo: false
parameters::model_parameters(efa) %>% kable() %>% 
  kable_styling(font_size = 24) %>%
 column_spec(8:8, color = "white",
              background = "red")
```
:::

## Variance accounted for

```{r}

efa$Vaccounted %>%
  knitr::kable()
```

## Path diagram

```{r}
#| fig-align: "center"
#| fig-width: 14
#| fig-height: 12

structure_big5 <- psych::fa(data, nfactors = 5, rotate = "none", fm="pa")%>%
  parameters::efa_to_cfa() # get lavaan code for fa

big5 <- suppressWarnings(lavaan::cfa(structure_big5, data = data)) # fit sem 

semPaths(big5, "std", layout="tree", nDigits = 3, label.cex=.5, edge.label.cex = .7, fade=FALSE) # plot the path model 

```

## Rotation

![](images/IMG_2403.png)

## Rotation

-   Make more interpretable (understandable) without actually changing the relationships among the variables

    -   Makes high loadings higher and low/medium loadings lower
        -   *Simple structure*
            -   High correlations and low correlations on each factor, but no medium correlations

## Rotation

-   Different types of rotation:

    -   Orthogonal Rotation (e.g., Varimax)

        -   This method of rotation prevents the factors from being correlated with each other

            -   Rotates the axes at 90 degrees

        -   Useful if you have factors that should theoretically be unrelated

    -   Oblique rotation (e.g., Direct Oblimin)

        -   Allows factors to correlate (more common)
        -   Good idea to always use this

## Rotation

::: columns
::: {.column width="50%"}
-   Orthogonal

![](images/IMG_2404.png)
:::

::: {.column width="50%"}
-   Oblique

![](images/IMG_2405.png)
:::
:::

## Rotation

-   Set `rotation` argument in `psych::fa`

```{r}
#change rotate arg to desired rotation
#orthogonal rotation
efa_or <- psych::fa(data, nfactors = 5, rotate="varimax", fm="pa") 

# correlated factor rotation
efa_obs <- psych::fa(data, nfactors = 5, rotate="oblimin", fm="pa")
```

## Rotation

-   After rotation

```{r}
#| fig-align: "center"
structure_big5 <- psych::fa(data, nfactors = 5, rotate = "oblimin", fm="pa")%>%
  parameters::efa_to_cfa() # get lavaan code for fa

big5 <- suppressWarnings(lavaan::cfa(structure_big5, data = data)) # fit sem 

semPaths(big5, "std", layout="tree", nDigits = 3, label.cex=.5, edge.label.cex = .7, fade=FALSE) # plot the path model 
```

## Rotation

-   For interpretable factor solution the convention is to eliminate small correlations ($r$ \< .32)

    -   Only explains 10% of the variance

-   Can set `threshold` argument to "max" if \< .32 does not produce interpretable factors

```{r}
#| eval: false
# correlated factor rotation
efa_obs %>% 
   model_parameters(sort = TRUE, threshold = .32)# can set to max if .3 does not lead to interpretable factors
#msx
efa_obs %>% 
   model_parameters(sort = TRUE, threshold = "max")# can set to max if .3 does not lead to interpretable factors
```

## Naming factors

-   PA1, PA2, etc probably not good factor names

-   Give factors intuitive names/labels

    -   Highly subjective!

    -   Use the highest loaded items to name factors

## Naming factors

-   Setting threshold to max works pretty well

```{r}
efa_obs %>% 
   model_parameters(sort = TRUE, threshold = "max") %>% print_html()

```

## What makes a good factor?

-   Makes sense

    -   Loadings on the same factor do not appear to measure completely different things

-   Easy to interpret

    -   Simple structure

        -   Contains either high or low loadings with few moderately sized loadings
        -   Lacks cross-loadings
            -   You don't have items that load equally onto more than 1 factor
                -   Keep items \> .32 and delete items \< .32 (use max loadings)

-   3 or more indicators per latent factor

## Factor scores

-   Estimated scores for each participant on each underlying factor (standing on factor)

    -   Standardize the factor loadings by dividing each loading by the square root of the sum of squares of the factor loading for that factor

    -   Multiply scores on each item by the corresponding standardized factor loading and then summing across all items

-   Can use them in multiple regression!

```{r}

efa_obs <- psych::fa(data, nfactors = 5, rotate="oblimin", fm="pa", scores="regression")

```

## Factor scores

Geller, J., Thye, M., & Mirman, D. (2019). Estimating effects of graded white matter damage and binary tract disconnection on post-stroke language impairment. *NeuroImage*, *189*. <https://doi.org/10.1016/j.neuroimage.2019.01.020>

![](images/CorrelationMatrix+FactorLoadings_Figure.png){fig-align="center"}

## Plotting factor analysis

::: columns
::: {.column width="50%"}
```{r}

# correlated rotation
efa_obs <- psych::fa(data, nfactors = 5, rotate="oblimin", fm="pa") %>% 
   model_parameters(sort = TRUE, threshold = "max")

efa_plot <- as.data.frame(efa_obs) %>%
  pivot_longer(PA2:PA4) %>%
  dplyr::select(-Complexity, -Uniqueness) %>% rename("Loadings" = value, "Personality" = name)


#For each test, plot the loading as length and fill color of a bar
# note that the length will be the absolute value of the loading but the 
# fill color will be the signed value, more on this below
efa_fact_plot <- ggplot(efa_plot, aes(Variable, abs(Loadings), fill=Loadings)) + 
  facet_wrap(~ Personality, nrow=1) + #place the factors in separate facets
  geom_bar(stat="identity") + #make the bars
  coord_flip() + #flip the axes so the test names can be horizontal  
  #define the fill color gradient: blue=positive, red=negative
  scale_fill_gradient2(name = "Loading", 
                       high = "blue", mid = "white", low = "red", 
                       midpoint=0, guide=F) +
  ylab("Loading Strength") + #improve y-axis label
  theme_lucid(base_size=18)

```
:::

::: {.column width="50%"}
```{r}
#| fig.align: "center"
#| fig.width: 12
#| fig.height: 8
#| 
efa_fact_plot
```
:::
:::

## Table FA

```{r}
source("https://raw.githubusercontent.com/franciscowilhelm/r-collection/master/fa_table.R")

efa_obs <- psych::fa(data, nfactors = 5, rotate="oblimin", fm="pa")

table<- fa_table(efa_obs)
```

## FA table

```{r}
#| echo: true
table$ind_table
```

## Confirmatory factor analysis

-   Do not do a confirmatory analysis with the same data you performed your exploratory analysis!

    -   Machine learning approach

-   Partition data training and test data

```{r}
# to have reproducible result, we will also set seed here so that similar
# portions of the data are used each time we run the following code
partitions <- datawizard::data_partition(data, training_proportion = 0.7, seed = 111)
training <- partitions$p_0.7
test <- partitions$test


```

## CFA in Lavaan

Let's compare the big6 to the big5

```{r}

structure_big5 <- psych::fa(training, nfactors = 5, rotate = "oblimin") %>%
  efa_to_cfa()

# Investigate how the models look
structure_big5

```

## CFA in Lavaan

```{r}
structure_big6 <- psych::fa(training, nfactors = 6, rotate = "oblimin") %>%
  efa_to_cfa()

structure_big6


```

## Fit and compare models

```{r}

big5 <- suppressWarnings(lavaan::cfa(structure_big5, data = test))
big6 <- suppressWarnings(lavaan::cfa(structure_big6, data = test))

performance::compare_performance(big5, big6, verbose = FALSE) %>%
  print_md()

```

## Information to include in paper

![](images/FA_guidelines.png){fig-align="center"}

## Write-up

::: columns
::: {.column width="50%"}
-   Factorablity

    -   KMO

    -   Bartlett's test

    -   Determinant of correlation matrix

-   Number of components

    -   Scree plot

    -   Eigenvalues \> 1

    -   Parallel analysis

    -   Agreement method
:::

::: {.column width="50%"}
-   Extraction method

    -   PAF

<!-- -->

-   Type of rotation

    -   orthogonal or oblique

-   Factor loadings

    -   Place in table or figure
:::
:::

## Sample write-up

::: callout-note
First, data were screened to determine the suitability of the data for this analyses. The Kaiser-Meyer- Olkin measure of sampling adequacy (KMO; Kaiser, 1970) represents the ratio of the squared correlation between variables to the squared partial correlation between variables. KMO ranges from 0.00 to 1.00 -- values closer to 1.00 indicate that the patterns of correlations are relatively compact and that component analysis should yield distinct and reliable components (Field, 2012). In our dataset, the KMO value was .86, indicating acceptable sampling adequacy. The Barlett's Test of Sphericity examines whether the population correlation matrix resembles an identity matrix (Field, 2012). When the p value for the Bartlett's test is \< .05, we are fairly certain we have clusters of correlated variables. In our dataset, χ1(300)=1683.76,p\<.001, indicating the correlations between items are sufficiently large enough for principal components analysis. The determinant of the correlation matrix alerts us to any issues of multicollinearity or singularity and should be larger than 0.00001. Our determinant was 0.00115 and, again, indicated that our data was suitable for the analysis.
:::

## Sample write-up

::: callout-note
Several criteria were used to determine the number of components to extract: a priori theory, the scree test, the eigenvalue-greater-than-one criteria, and the interpretability of the solution. Kaiser's eigenvalue-greater-than-one criteria suggested four components, and, in combination explained 49% of the variance. The inflexion (elbow) in the scree plot justified retaining four components. Based on the convergence of these decisions, four components were extracted. We investigated each with orthogonal (varimax) and oblique (oblimin) procedures. Given the non-significant correlations (ranging from -0.03 to 0.03) and the clear component loadings in the orthogonal rotation, we determined that an orthogonal solution was most appropriate.
:::

## Factor analysis: Summary

-   What:

    -   Identifies where the most variance is in your data in smallest number of factors

-   When:

    -   Your data has many measures. Almost too many to interpret
    -   Measures are correlated
    -   Phenomenon cannot be directly tested

-   Why:

    -   Simplfy data
    -   Identify/test underlying constructs (factors)

## Class Announcements

-   Wednesday is last lab of the semester

-   All lab revisions due end of reading week (you will not get a final grade from me until they are turned in)

-   Blog post due May 13th
